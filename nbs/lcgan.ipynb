{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1711.05772 / https://arxiv.org/abs/1802.04877\n",
    "\n",
    "https://github.com/natashamjaques/magenta/blob/affective-reward/magenta/models/affective_reward/latent_gan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda.is_available: True\n",
      "available: 1; current: 0\n",
      "cuda:0\n",
      "pytorch 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('cuda.is_available:', torch.cuda.is_available())\n",
    "print(f'available: {torch.cuda.device_count()}; current: {torch.cuda.current_device()}')\n",
    "DEVICE = torch.device(f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "print('pytorch', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pdb import set_trace\n",
    "#set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = np.empty([0, 2], dtype=int)\n",
    "gens_keep = np.empty([0, ], dtype=object)\n",
    "zs_keep = np.empty([0, 1, 128], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/iconix/pytorch-text-vae.git\n",
    "from pytorchtextvae import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cached info at ../../pytorch-text-vae/model/best/tmp/reviews_and_metadata_5yrs_stored_info.pkl\n",
      "Cache ../../pytorch-text-vae/model/best/tmp/reviews_and_metadata_5yrs_stored_info.pkl loaded (load time: 0.62s)\n",
      "Found saved model ../../pytorch-text-vae/model/best/reviews_and_metadata_5yrs_state.pt\n",
      "MAX_SAMPLE: False; TRUNCATED_SAMPLE: True\n",
      "Trained for 360000 steps (load time: 18.88s)\n",
      "Setting new random seed\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cpu') # CPU inference\n",
    "n_samples = 100\n",
    "temp = 0.2\n",
    "\n",
    "# workaround for un-pickling after module directory change https://stackoverflow.com/a/45264751\n",
    "#import sys\n",
    "#sys.path.append('../../pytorch-text-vae/pytorchtextvae')\n",
    "\n",
    "vae, input_side, output_side, pairs, dataset, EMBED_SIZE, random_state = generate.load_model('../../pytorch-text-vae/model/best/reviews_and_metadata_5yrs_state.pt', 'reviews_and_metadata_5yrs_stored_info.pkl', DEVICE, cache_path='../../pytorch-text-vae/model/best/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "the band seems to grew up over the past and its also on the r b side by heavy side of releases\n",
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "the young love from a deep deep and to as much of a new sense to move through these relationship\n",
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "were too familiar with nearly UNK no UNK when just announced a debut this is coming as part of the as part of UNK months\n",
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "UNK UNK UNK is it with a couple of five years before releasing their debut album since they fall over the last fall\n",
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "like this which seems to be the cool UNK UNK mark as an tough time\n",
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "in the time when we listen to and his own part of love when we fell in love for an all night night hit\n",
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "up at the end of the day this has such where slowly leading up to this six months\n",
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "on the UNK years since i picked up on a bit UNK UNK from swedish artists styles as artist voice\n",
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "artist is at the any of the world as though and if there as the kind of being sounds them\n",
      "---\n",
      "['downtempo', 'dream pop', 'indietronica']\n",
      "one of the north american tour said with latest album if you to get some for his latest single\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['the band seems to grew up over the past and its also on the r b side by heavy side of releases',\n",
       "  'the young love from a deep deep and to as much of a new sense to move through these relationship',\n",
       "  'were too familiar with nearly UNK no UNK when just announced a debut this is coming as part of the as part of UNK months',\n",
       "  'UNK UNK UNK is it with a couple of five years before releasing their debut album since they fall over the last fall',\n",
       "  'like this which seems to be the cool UNK UNK mark as an tough time',\n",
       "  'in the time when we listen to and his own part of love when we fell in love for an all night night hit',\n",
       "  'up at the end of the day this has such where slowly leading up to this six months',\n",
       "  'on the UNK years since i picked up on a bit UNK UNK from swedish artists styles as artist voice',\n",
       "  'artist is at the any of the world as though and if there as the kind of being sounds them',\n",
       "  'one of the north american tour said with latest album if you to get some for his latest single'],\n",
       " [tensor([[-1.7998,  0.9456,  0.5475,  0.6110,  0.0051, -0.4701, -1.2076,\n",
       "            0.1346, -1.7347, -1.7929,  1.2106,  0.7643,  0.1680, -1.6990,\n",
       "           -1.1616,  1.2473, -0.0646, -0.0189,  0.1641,  0.1150, -0.2883,\n",
       "           -0.4711, -2.0062,  1.4461,  0.2009, -0.5751, -2.0193,  1.1039,\n",
       "            0.1243,  0.0051,  0.2056,  0.1951, -1.4505,  1.5605,  1.2725,\n",
       "           -1.3519, -0.7507, -0.6215, -0.0485, -0.6197,  0.2886,  0.4406,\n",
       "           -1.0085, -0.1267, -0.8785, -1.4849, -0.5115,  1.1326, -0.9150,\n",
       "            0.1650,  2.2356,  0.5115,  0.5730,  1.1597,  0.2592,  1.7888,\n",
       "           -0.8877,  0.2212,  0.8480, -0.5127, -0.8395,  2.0541,  0.0372,\n",
       "           -1.4402, -0.9120, -0.7167, -0.3695,  0.7964,  1.0748,  0.9459,\n",
       "           -0.0882, -0.8576,  0.0639, -0.7402,  0.8048, -0.3906,  1.5735,\n",
       "            0.1402, -0.6810, -0.8819,  0.6992,  0.7337, -1.5266, -0.1880,\n",
       "           -0.7316, -0.4504,  2.2113, -0.3856,  3.4571,  1.9277, -0.9423,\n",
       "            0.5616, -0.3161, -0.6946,  0.9908, -0.8680,  0.8493, -0.6942,\n",
       "            1.0253,  1.1374, -0.6779,  0.7060, -0.1098,  0.3954, -0.4259,\n",
       "           -0.9898,  0.3319, -1.4856,  1.0190,  1.7949,  1.5216, -1.1204,\n",
       "            2.0737, -0.2279,  0.6128, -0.9006,  0.7129,  0.8398,  0.5831,\n",
       "           -0.4514, -0.0675,  1.7794,  0.0732, -0.1430,  0.0483, -0.5200,\n",
       "           -0.3556,  0.1980]]),\n",
       "  tensor([[-0.4966, -1.0580,  1.3030, -0.8353,  0.7727,  0.4598, -0.2160,\n",
       "            0.3982, -0.5309,  0.5724, -0.3169, -1.6047,  0.4291, -0.3770,\n",
       "           -1.8112,  1.5978,  0.8818, -0.4541, -0.0991, -0.3813,  0.3413,\n",
       "           -0.5982, -0.3728,  0.5396, -0.6256, -0.0994,  0.4499, -0.0494,\n",
       "            1.9005,  0.6563,  0.1585,  0.3165,  0.3030,  0.7731,  0.2699,\n",
       "           -1.1497, -2.2608,  0.2486, -0.4490, -0.8712,  1.9628, -0.2349,\n",
       "           -0.5426, -0.4182,  0.0261, -1.6642, -1.4110,  0.4987,  0.3643,\n",
       "           -0.1997, -0.8809,  0.0199, -0.4266, -1.2853, -0.4746,  0.1321,\n",
       "            0.4680, -0.4141,  0.3652,  0.1603,  1.3865, -0.6089,  0.0411,\n",
       "           -0.5578, -0.4412, -1.7473,  0.7859,  0.3194, -0.3537, -0.6210,\n",
       "           -0.6397,  0.1751, -1.5409,  0.8672,  0.1784,  1.1832,  0.1431,\n",
       "            0.7631,  0.0993,  0.9792,  0.2358,  0.8823, -0.3579,  0.5327,\n",
       "            0.0857,  0.2474,  1.7078, -1.4118, -0.3449, -0.7404, -0.2035,\n",
       "            0.7413,  1.4793, -0.4839, -1.0131,  0.0624,  0.5736,  0.8522,\n",
       "           -0.2001,  0.3422,  0.3301, -1.5849, -0.1901, -1.1396, -2.0200,\n",
       "            0.1614, -1.1887, -0.8338,  0.1423,  0.4815,  1.3577,  0.5521,\n",
       "           -0.5792, -1.7108, -0.6071, -1.3049, -0.1059,  2.9221,  0.1491,\n",
       "           -0.0311,  1.0136, -0.3756, -0.3058, -0.6332,  0.8024, -0.4196,\n",
       "           -0.1540,  0.7722]]),\n",
       "  tensor([[ 1.1009,  0.6772, -1.0736,  1.9884,  1.1384, -0.8081,  0.5334,\n",
       "           -0.3924,  2.1677, -0.5074, -0.4337, -0.6605, -0.1023,  0.2450,\n",
       "            1.3940, -0.4847,  0.4638,  0.6994,  0.4251,  1.0152,  0.6006,\n",
       "            1.0758, -0.0316, -0.8777, -0.2214, -0.7711, -0.3970, -0.6103,\n",
       "            0.7086, -0.8299, -0.6027,  0.6681, -0.1555,  0.2767,  0.3574,\n",
       "           -0.0469, -0.4340,  1.9536,  1.1186,  0.4938,  1.1442, -0.2123,\n",
       "            1.1815, -0.6307,  0.5289,  0.2502, -0.7173,  0.2877,  0.7346,\n",
       "            0.1162, -0.1508, -0.3166, -0.5190, -1.7660,  0.7124, -0.4773,\n",
       "            0.5153, -1.1283, -0.5656,  0.4817, -0.2792,  0.8506,  0.2959,\n",
       "            1.7241, -0.4161, -0.0153, -1.2517, -1.1630, -1.9939, -0.0393,\n",
       "            1.2190, -0.2862,  1.0560, -0.0723,  0.2699, -0.4868,  0.8990,\n",
       "            1.3665,  0.9473, -0.7024, -0.6519, -0.0293,  1.0120, -0.1313,\n",
       "            1.3673,  0.7424, -1.0114,  1.1001,  1.0056, -0.4679, -1.4267,\n",
       "           -0.3493,  0.1259, -1.7092, -0.7278, -0.2241,  0.2552,  0.9592,\n",
       "            0.5046, -0.5311,  1.0824,  1.0582, -1.0746, -0.6573, -1.6610,\n",
       "           -0.3828, -0.1457, -0.7425, -1.8849, -1.9662,  1.5730, -0.2327,\n",
       "           -1.1522,  1.6386, -0.1302,  0.8920, -0.4962,  1.8326, -0.7274,\n",
       "            0.8232, -0.8554,  1.0898,  1.8651,  0.0749, -0.4434,  1.1672,\n",
       "            0.6076,  0.2771]]),\n",
       "  tensor([[-0.0375,  0.2225,  0.5594,  0.0378,  1.3821,  0.0470,  0.2233,\n",
       "            1.2428,  0.3087, -0.0730,  0.2694, -1.6513, -1.7676,  0.1376,\n",
       "            1.1293, -0.8774, -0.0362,  0.2907, -2.4708, -0.6598,  0.1716,\n",
       "           -1.4171, -1.0266, -0.1793, -0.0996,  0.6326,  2.6788,  0.3735,\n",
       "            0.7044, -1.8030,  0.7099, -0.0867,  0.9126, -0.4227,  0.1176,\n",
       "           -0.3369,  1.9116, -0.4298, -0.1544, -0.0212, -0.6915,  1.0441,\n",
       "           -0.1251, -0.7306, -0.5808, -0.2120,  1.2183, -0.9525,  0.4897,\n",
       "            1.1583,  1.1205,  1.0985, -1.1398,  0.4569,  0.1894, -1.3418,\n",
       "           -0.4988, -1.1682, -0.6009,  2.7722,  1.1822, -0.3629,  0.6263,\n",
       "            0.6770, -0.6880,  0.3053, -0.1438, -0.6779,  0.3852, -0.0033,\n",
       "           -0.5072, -0.3057,  2.4301,  1.1836,  0.8445,  1.4887,  0.3061,\n",
       "           -1.0585, -1.5768, -0.4739,  0.3794,  0.6200,  2.0169, -1.5509,\n",
       "            0.7087,  0.0699,  1.5515, -0.3492, -0.0358,  0.3508, -0.4698,\n",
       "           -0.3547,  0.4662,  0.9891, -1.5478,  0.0470, -1.0701,  1.0058,\n",
       "           -0.9901,  0.1607,  0.2105, -0.1486,  0.2012, -0.2960,  0.0745,\n",
       "            0.8154,  0.4575, -0.3435, -0.6014, -0.7940, -0.2775,  0.2001,\n",
       "           -0.1699,  0.3744, -0.6117,  0.6908, -0.3971,  0.5510,  0.8692,\n",
       "            0.0528,  0.0824, -1.0865, -1.8487, -1.0121,  0.3664,  0.5959,\n",
       "           -0.8720, -0.5297]]),\n",
       "  tensor([[-0.8698, -0.0772, -0.4525, -0.8069,  0.0097, -1.6142,  0.2003,\n",
       "           -0.7291, -1.2696, -0.4233, -1.6556,  1.6899,  0.1384,  1.2292,\n",
       "            1.4174,  0.3504, -0.0354,  0.7135, -2.4023, -0.3413, -2.7751,\n",
       "           -0.1323,  1.2632, -0.0407,  2.5209, -0.2126,  0.4014,  0.0882,\n",
       "            2.1435,  1.5702, -0.5252,  1.0337,  0.1261,  0.6449, -1.4197,\n",
       "            0.9049, -0.1832, -0.2732,  0.8581,  0.8932,  0.4006,  0.6699,\n",
       "           -0.7725, -1.3076, -0.1919,  0.9105,  1.6295, -1.1357,  0.5116,\n",
       "            0.5446,  0.5997,  0.4582, -1.0152, -2.3582,  1.4483, -1.4015,\n",
       "           -0.3983,  1.2453, -0.2312, -2.1339,  0.7773, -2.3985,  0.6227,\n",
       "           -1.1677,  1.8449, -0.2357,  0.4260,  0.2522,  1.5289,  0.5313,\n",
       "            1.9772, -1.8911,  0.7929,  1.2613, -0.7326,  2.9601, -0.1482,\n",
       "            0.2015,  0.7864, -0.9328,  1.5412,  0.3421,  0.4161, -0.2621,\n",
       "           -0.4464,  1.5884,  0.8534,  0.4869,  1.6857,  0.0358, -0.8610,\n",
       "           -2.4055, -0.2529, -2.5836,  0.8906,  0.0846, -0.0385,  1.4817,\n",
       "           -0.1911,  0.3964, -0.5737, -0.1213, -1.6014, -1.5601,  0.7696,\n",
       "           -0.6515, -0.7470,  0.4009,  1.4805, -0.4370,  0.0874,  0.4682,\n",
       "            1.9852,  0.6421,  1.4331,  0.0405, -0.9895, -0.0432, -0.9617,\n",
       "           -1.1929,  0.6064, -1.3592, -0.3346, -2.1728,  1.8888, -0.5721,\n",
       "           -0.6792,  0.0028]]),\n",
       "  tensor([[-0.0192, -1.3437,  2.0075, -1.0278, -0.9921, -1.4813,  0.9492,\n",
       "            0.2729,  0.6458,  0.4963,  0.5369,  1.9125,  0.5952,  0.2497,\n",
       "            0.7643,  1.2609, -1.4567, -0.7445,  0.0532, -0.8713,  0.0868,\n",
       "           -0.4326, -0.6764, -1.2482, -0.8275,  0.8864, -0.2624,  0.4525,\n",
       "            0.1458, -1.0140, -1.9114, -1.6497, -1.1177, -0.4017,  0.9193,\n",
       "           -2.1880,  0.6149, -1.4843,  0.7826,  0.1212,  0.9679, -1.6652,\n",
       "            1.2527,  0.8212, -0.4965, -0.5426,  0.1272, -1.4890, -0.4467,\n",
       "           -1.5172,  0.5922, -0.3373, -1.8919, -0.7434, -1.1737, -0.6565,\n",
       "            1.0895, -0.3303, -0.4701,  0.5990,  0.1851, -0.4773, -0.0172,\n",
       "           -1.0078,  2.2209,  0.9089,  0.2968, -1.5727, -1.2354, -0.4425,\n",
       "            0.1409,  2.3274,  0.9594, -2.1168,  1.2852,  2.1763, -0.0741,\n",
       "           -0.0223, -0.0652, -1.6135,  0.1432, -0.1260, -0.1164,  0.6459,\n",
       "            1.0847, -1.0990, -0.8375,  1.4745,  0.0965,  1.1299,  0.6317,\n",
       "            0.7194, -0.5687, -1.2230, -0.4373,  0.0185, -0.0611, -0.3105,\n",
       "            0.4997, -1.4263,  1.7287, -2.1891,  0.3581, -0.1290,  0.8123,\n",
       "            0.0852, -0.0169, -0.6809, -1.5181,  0.7184,  0.9304, -0.9317,\n",
       "           -2.7392,  1.5857,  1.3478,  1.9760,  0.2655, -1.5958, -0.6940,\n",
       "           -0.4655, -0.0303,  0.4443, -0.4158, -1.7471,  0.8304, -0.0720,\n",
       "            1.2367,  0.1015]]),\n",
       "  tensor([[-0.7364,  1.6465, -0.2116,  1.9903,  0.5608,  1.6784, -1.8971,\n",
       "           -0.7103, -1.9576, -1.5161,  0.3040,  1.5338,  1.7343, -0.7024,\n",
       "            1.1378, -0.5993, -0.3995,  3.6068,  0.4866,  0.5486,  1.0340,\n",
       "           -2.4857, -0.5119, -2.2750,  0.0925, -0.0436,  1.2055, -0.4746,\n",
       "            1.0056,  0.3335,  0.1437, -1.6311, -1.2504,  0.2095, -0.2804,\n",
       "            0.1125, -2.1484, -0.6177,  1.0153, -1.7819, -0.0369,  1.4566,\n",
       "            0.0116, -1.7790,  0.4276, -0.1045, -2.0154,  0.3729,  1.2094,\n",
       "           -0.4637, -0.5077,  0.3157, -0.0261,  0.1372, -1.3021, -0.2882,\n",
       "            0.3410, -1.0569,  0.1589, -1.2495,  0.6853, -0.4318, -0.3831,\n",
       "           -1.0666, -1.4699, -1.1833,  1.9692,  0.9062, -1.1295,  0.6614,\n",
       "           -0.8766,  0.7979, -1.4116, -0.8545, -1.4284,  1.2408, -0.6010,\n",
       "            0.4801, -1.6744,  0.0359, -1.6217,  1.0990, -0.2475,  1.4234,\n",
       "           -1.3731, -1.0889, -1.2321,  0.6756,  0.5598,  0.2298,  0.8339,\n",
       "           -1.4071,  0.0811, -0.7944, -0.1960,  1.2232,  0.0865, -0.8593,\n",
       "           -1.5598,  1.1791, -1.3125,  0.9775,  0.8843,  0.7909, -0.1301,\n",
       "            3.0439,  0.6132, -0.8074, -0.5298, -1.2243, -0.2028,  2.4015,\n",
       "           -0.4437,  0.8848,  0.0961,  0.1240,  0.4898,  0.2366, -1.0167,\n",
       "            0.4425,  0.7316, -0.5409,  0.1508,  0.7446,  1.7460,  0.5293,\n",
       "            0.9768,  0.0692]]),\n",
       "  tensor([[-1.2332, -1.0242, -0.8033,  0.0971, -0.9984,  0.6042,  0.5981,\n",
       "            1.0686, -0.3506,  1.1018, -0.5293,  0.6961, -0.8248,  0.0496,\n",
       "            1.2140,  0.1259,  0.3418,  0.0525,  1.4798, -0.2389,  0.1063,\n",
       "            1.0080, -0.9689,  0.8566, -1.4962, -1.8088, -2.0830,  2.1170,\n",
       "            0.2542,  0.0117,  2.1667, -0.8409,  0.2673, -0.6189,  0.0396,\n",
       "            1.3516,  0.4933, -0.0496, -0.8976,  1.2350,  1.3055, -1.3443,\n",
       "            0.5168, -0.4431, -1.0408, -0.0439, -0.9498,  1.2931,  1.3761,\n",
       "           -0.0198,  0.8742, -0.2305, -2.4996, -0.5734, -1.5925, -0.2678,\n",
       "           -0.0514, -1.5863,  0.5627,  1.2036,  1.0567, -0.8321, -0.9991,\n",
       "           -0.1771,  1.4588,  0.8014, -0.0702, -0.0281, -2.0980,  0.8070,\n",
       "           -0.0903, -1.1974, -0.2977,  1.1756, -0.0625, -0.4113,  1.3524,\n",
       "           -0.1974, -1.2017, -0.5068, -0.8140, -0.4938, -0.2119,  0.1028,\n",
       "           -0.2874,  0.6383,  0.1576, -0.7509,  0.1333, -0.5222,  1.5896,\n",
       "            1.3975,  0.6797, -0.7030,  0.7283,  1.2705,  0.3299,  1.8402,\n",
       "            0.7694,  0.8443, -0.1845,  0.5065,  0.1027, -1.5084, -0.8343,\n",
       "            0.5176, -1.3259, -0.5123,  1.2997,  0.5769, -0.8326,  0.2971,\n",
       "            0.0648, -0.8809, -0.2290, -0.8863, -0.9298,  1.0950,  0.9190,\n",
       "            2.6950, -2.2097,  1.0658,  1.2128,  1.1207, -0.5003,  2.4359,\n",
       "           -1.6900,  2.0832]]),\n",
       "  tensor([[ 1.1612, -0.5327, -0.0778, -0.6230,  0.5992, -0.0201, -1.1417,\n",
       "            2.1243, -0.7677, -0.2704,  0.6148, -0.6909,  0.4760,  0.2643,\n",
       "            0.4189, -1.8494,  1.6559,  0.3203, -0.2442, -2.2713,  0.2777,\n",
       "            0.0420, -1.5143,  0.0072, -0.1483, -0.4487, -1.9226, -0.1564,\n",
       "           -0.3075, -0.5354,  1.8360,  1.2017,  1.8209, -0.1750,  0.7582,\n",
       "           -0.0183,  1.9586, -1.2730,  0.6126,  0.3826, -0.7056, -0.1922,\n",
       "           -1.5743, -0.0768,  0.4017, -0.1474,  1.0446,  0.6056,  0.3718,\n",
       "           -0.8341, -0.5061,  0.7508, -0.6023,  0.9861, -1.3110, -0.2661,\n",
       "            1.5881, -0.5750,  0.0427, -0.1587,  0.7988,  0.0497, -0.6547,\n",
       "            0.0407,  0.0926, -0.0132, -0.8674,  0.1784, -0.5185,  1.1393,\n",
       "            1.1044, -1.7052,  0.0328, -2.6263, -1.0982,  0.8424,  0.6446,\n",
       "            1.9831, -0.3114, -3.4150,  0.3111,  0.5917, -1.4242,  0.8407,\n",
       "           -2.1038,  0.1884, -0.3738,  1.1049,  0.9649, -0.3561, -1.9973,\n",
       "           -0.2164,  0.1811,  0.6266, -0.0069, -0.3175,  0.3916,  1.3103,\n",
       "            1.0017,  0.0241,  0.2922,  0.6401,  0.6409, -0.1695,  1.2337,\n",
       "           -0.2770,  0.1281,  1.8271,  0.1439, -0.3691, -0.0493,  0.4279,\n",
       "            0.6044, -0.9962, -1.6071, -0.9182, -2.1536, -0.9784, -0.4119,\n",
       "           -0.0323, -0.7438,  0.8328,  0.8352,  0.0409, -0.0790, -0.7479,\n",
       "           -0.2127, -0.6315]]),\n",
       "  tensor([[-0.0246, -0.5551, -0.0390, -1.9770, -0.2858, -0.5164,  0.7203,\n",
       "            0.7774,  0.4041, -0.3228,  0.5231,  2.1400, -0.8543, -0.7914,\n",
       "           -0.5263, -1.3567, -0.7100,  0.1974,  1.1747,  2.2740, -0.0606,\n",
       "            1.1634,  0.2640,  0.4807, -1.0011, -0.1706, -2.0813,  1.2377,\n",
       "            0.0384, -0.2959, -0.5927, -0.4963, -0.1504,  1.3832, -0.8037,\n",
       "            2.9947,  0.6571,  0.1464, -0.0731, -0.0021, -2.9594, -1.4668,\n",
       "            0.8327,  0.7038, -0.4215, -0.4102,  0.1775, -1.1695, -0.2619,\n",
       "           -0.2382, -0.9718, -1.5210, -0.6282, -0.5975,  1.9476, -0.5497,\n",
       "           -0.8329,  0.1560,  0.3857,  1.4460,  0.3564,  0.3590,  0.4315,\n",
       "           -0.5201, -0.2915,  0.3543,  1.8700, -0.5352, -0.3153,  1.8312,\n",
       "           -0.8850,  0.8524,  0.0421, -0.4736,  0.3804,  0.6702,  0.8090,\n",
       "           -0.1866,  0.1164, -1.1112, -1.1493, -0.1446, -0.3620, -0.0878,\n",
       "            0.4468, -0.5204, -0.4440, -0.7454,  0.3618,  0.4873,  0.2528,\n",
       "            0.1034,  0.4758,  0.8484, -0.7757,  0.2325,  1.2913, -0.6860,\n",
       "           -0.1630, -0.0499,  0.6239,  0.2874, -0.4489,  0.6223,  1.1242,\n",
       "            0.2751,  0.3665,  0.6174,  0.5260,  0.4883, -0.2954, -0.5791,\n",
       "           -1.0245, -0.7195, -0.6021, -0.0868,  0.3572, -0.9209, -0.4475,\n",
       "           -0.0005, -0.7692, -0.6116, -1.3590,  0.7619,  1.7575, -0.2660,\n",
       "            1.1003, -1.6196]])],\n",
       " [tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       "  tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "            1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate.generate(vae, input_side, output_side, pairs, dataset, EMBED_SIZE, random_state, DEVICE, genres=['downtempo', 'dream pop', 'indietronica'], num_sample=10, temp=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "['chamber pop', 'indie anthem-folk', 'indie electro-pop', 'vapor pop', 'vapor soul']\n",
      "theres a new sounds at the there with a emotive and back in addition to each audience\n",
      "---\n",
      "['indie r&b', 'vapor soul']\n",
      "not sure youve come of of UNK seems to released a couple of UNK earlier this years collaboration\n",
      "---\n",
      "['edm', 'moombahton', 'ninja', 'pop']\n",
      "now theyve released a five piece of us and their music vocals for a dream pop piece at music below\n",
      "---\n",
      "['indietronica']\n",
      "electro UNK UNK UNK according to the track itself into a different emotions and available on this weekend\n",
      "---\n",
      "['conscious hip hop', 'gangster rap', 'hip hop', 'pop rap', 'rap', 'southern hip hop', 'trap music', 'underground hip hop']\n",
      "UNK UNK UNK UNK UNK used to look at UNK the old and based in start of this coming today\n",
      "---\n",
      "['alternative dance', 'chillwave', 'indietronica', 'nu disco', 'shiver pop', 'vapor soul']\n",
      "felt like this image courtesy of artist and its sort of feels like track right\n",
      "---\n",
      "['indie r&b']\n",
      "he started with the remix of artists such as he has made in an electronic songs that could made the top on the songs\n",
      "---\n",
      "['deep tropical house', 'indie poptimism', 'tropical house', 'vapor soul']\n",
      "producer artist sounds in say by these two track and check out than most recently\n",
      "---\n",
      "['indie psych-rock']\n",
      "artist continues to is taken from the album but theyve returned to a album that that sounded like find effect\n",
      "---\n",
      "['aussietronica', 'deep australian indie', 'electropop', 'indie r&b', 'indietronica', 'vapor soul']\n",
      "this is off from a few days ago and because of their this coming games\n",
      "---\n",
      "['funk']\n",
      "around the UNK below doesnt look the UNK and listen to UNK UNK or reminiscent of the his voice\n",
      "---\n",
      "['bass music', 'chamber psych', 'deep house', 'float house', 'fluxwork', 'future garage', 'microhouse']\n",
      "UNK just dropped an official remix of UNK track since we yet to moving away from the courtesy of electronic music together\n",
      "---\n",
      "['alternative dance', 'indie pop', 'indie poptimism', 'indie rock', 'indietronica', 'la indie', 'modern rock', 'shimmer pop', 'vapor soul']\n",
      "from the image courtesy of guy who continues to obsessed with everyone from different from the as i had to comes from this scene\n",
      "---\n",
      "['rock']\n",
      "until the UNK took your day and thought it would be featured on the last UNK cover\n",
      "---\n",
      "['big room', 'dance pop', 'edm', 'electro house', 'pop', 'tropical house']\n",
      "with only a bit of production and and youre a fan of the above to add a live instrumentation\n",
      "---\n",
      "['indietronica']\n",
      "back in UNK and with the new single from UNK who just released a follow up for the next album on coming soon\n",
      "---\n",
      "['australian alternative rock', 'australian dance', 'downtempo', 'pop']\n",
      "by artist recently released the official single from the UNK below and be sure to drop their upcoming album on late october\n",
      "---\n",
      "['house']\n",
      "image courtesy of artist as described as you above and follow official it for last time\n",
      "---\n",
      "['microhouse']\n",
      "artist is the a long way that way back in the kind after building on the new production\n",
      "---\n",
      "['bass music', 'escape room', 'fluxwork', 'future garage', 'indie r&b', 'microhouse', 'wonky']\n",
      "and behind much less from artist who lead able thanks to a less spot on pop instrumental\n",
      "---\n",
      "['gauze pop', 'tropical house']\n",
      "to look at the rest of the UNK is going i allows you to listen to to enjoy\n",
      "---\n",
      "['funk', 'soul']\n",
      "the track is itself in its a piece a groove pace with its its guitars and perfectly backing vocals\n",
      "---\n",
      "['indie r&b', 'vapor soul']\n",
      "in UNK UNK UNK UNK UNK has returned with the likes of UNK material since UNK who returns with the first single after a few years eps\n",
      "---\n",
      "['escape room', 'indie poptimism', 'la indie']\n",
      "away from the three piece impressed us with duos first single UNK which it features to james set\n",
      "---\n",
      "['aussietronica', 'shimmer pop', 'vapor soul']\n",
      "some UNK is different from the year the to to some reason why i feel every single\n",
      "---\n",
      "['chamber pop']\n",
      "UNK UNK at neo soul electronic and wont have signed to facebook june\n",
      "---\n",
      "['deep house', 'deep tropical house', 'house', 'nu disco', 'tropical house', 'vapor soul']\n",
      "the la the trio artist have released so many of us with a on tour and stay with your free\n",
      "---\n",
      "['aussietronica', 'deep australian indie']\n",
      "lost in the back in the soulful back and a slow bit of his previous seconds\n",
      "---\n",
      "['bass music', 'electronic trap', 'future garage', 'vapor twitch', 'wonky']\n",
      "i was working on a debut album but you and find out at the and that but with you with any just as just as as more\n",
      "---\n",
      "['alternative dance', 'canadian pop', 'electropop', 'indie poptimism', 'metropopolis', 'neo-synthpop']\n",
      "UNK came out with a big ovo sound artist and the kind of ovo sound kicks off\n",
      "---\n",
      "['electropop', 'indie electro-pop', 'indie poptimism', 'vapor soul']\n",
      "anna of UNK UNK UNK UNK for a slew of new years of the north returns with the new music which was anna of the last few years\n",
      "---\n",
      "['vapor soul']\n",
      "the ability to UNK UNK and that UNK is the seem to put them into one minute reason they made going on me set\n",
      "---\n",
      "['electropop', 'indie poptimism', 'indie r&b', 'modern rock', 'pop']\n",
      "but i have love gone and when i fell in love but thats worth my love lately\n",
      "---\n",
      "['indie r&b']\n",
      "the berlin french producer artist has to be UNK UNK UNK UNK i fell i love style\n",
      "---\n",
      "['big beat', 'downtempo', 'electronic']\n",
      "were re away from the likes of artist and stay ready for the over over before well before their third full length\n",
      "---\n",
      "['indie electro-pop', 'indie poptimism']\n",
      "my official anna of the UNK UNK UNK up on its an ethereal electro UNK vibe\n",
      "---\n",
      "['big room', 'edm', 'pop', 'tropical house']\n",
      "you you as a part of UNK who and you at the UNK UNK UNK started working on your previous work\n",
      "---\n",
      "['deep tropical house', 'edm', 'indie poptimism', 'tropical house', 'vapor soul', 'vapor twitch']\n",
      "they are playing out of the record and return to the show at the record that will take the back in side of the remix from there on march\n",
      "---\n",
      "['tropical pop edm']\n",
      "have an almost as one of the UNK and its almost an up and warm and melodic focus on their vocal vibe\n",
      "---\n",
      "['gauze pop']\n",
      "i have heard this song with the one of my friend and and in and and electric feel doing it take space\n",
      "---\n",
      "['electropop', 'indie electro-pop', 'indie poptimism', 'indie psych-rock', 'indie r&b', 'vapor soul']\n",
      "i was inspired by some of the UNK that its been remixed by UNK years ago since UNK explains\n",
      "---\n",
      "['vapor twitch']\n",
      "the the duo artist have taken from his forthcoming new ep which is part of my very lost in my love\n",
      "---\n",
      "['indie anthem-folk', 'indie r&b']\n",
      "UNK UNK UNK UNK UNK UNK is the friends and im at the room for a whole or chance\n",
      "---\n",
      "['indie anthem-folk', 'indie r&b', 'vapor soul']\n",
      "taken from artist and weve come since the release of UNK UNK UNK UNK will holding onto your no party\n",
      "---\n",
      "['vapor twitch']\n",
      "we always back to a new door cinema club which fit for a track and i feel and below along with some electro beats\n",
      "---\n",
      "['chillwave', 'indie dream pop']\n",
      "UNK UNK UNK UNK is away with the remix of UNK which were in the festival this weekend\n",
      "---\n",
      "['norwegian pop']\n",
      "UNK is UNK nothing and talking about the UNK UNK UNK to the music in a start of a UNK and direction that real relationship\n",
      "---\n",
      "['art pop', 'chamber psych', 'folk-pop', 'new americana', 'preverb']\n",
      "UNK is very much UNK UNK UNK UNK UNK and UNK UNK UNK UNK UNK UNK UNK UNK in a very guitars like the sound very much of the instrumentation and produced by james machine\n",
      "---\n",
      "['trap soul']\n",
      "feels like youre able to know there is the first time youre to make it right around your connect\n",
      "---\n",
      "['folk-pop', 'indie folk', 'indie pop', 'modern rock', 'neo mellow', 'new americana', 'stomp and holler']\n",
      "artist had a part of the UNK are combined of a UNK with you up in a bit for you while\n",
      "---\n",
      "['big room', 'complextro', 'edm', 'electro house', 'electropop', 'filter house']\n",
      "the first tracks to find you with the official remixes of the official remixes and and called rework below and grab it below and today\n",
      "---\n",
      "['indie electro-pop', 'indie poptimism', 'vapor soul']\n",
      "the track was produced too many artists world as while youve yet to keep two singles fast\n",
      "---\n",
      "['indie']\n",
      "you can also really clear that there are also over the kind of pop music from last UNK but it can also whats to come over the last we love that yet\n",
      "---\n",
      "['indie r&b']\n",
      "UNK UNK UNK UNK and UNK is an UNK of UNK and a UNK that has been working of the night since UNK release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "['indie pop', 'indie poptimism', 'indie psych-rock', 'indietronica', 'modern rock']\n",
      "this UNK UNK were working on the dark indie rock sound that is taking listeners into a on packed with soft and light as well featuring the front man himself\n",
      "---\n",
      "['alternative dance', 'big beat', 'dance-punk', 'electro house', 'electronic', 'indietronica', 'new rave']\n",
      "so much of the me as i saying that this UNK and open up of summer\n",
      "---\n",
      "['indie electro-pop']\n",
      "the the sounds of UNK is just with a UNK UNK with UNK songs style\n",
      "---\n",
      "['indie pop', 'indie poptimism', 'indie psych-rock', 'indietronica', 'modern rock', 'shimmer pop']\n",
      "this UNK UNK is heading to select festival in october in which which returns with the release the york month\n",
      "---\n",
      "['norwegian pop', 'pop']\n",
      "it feels right here in sultry voice that serve by a before and were both in their ear\n",
      "---\n",
      "['electropop', 'indie electro-pop', 'indie poptimism', 'indietronica', 'vapor soul']\n",
      "i was able to get right into the synths and it with the the right as the right sounds\n",
      "---\n",
      "['indie pop', 'indie poptimism', 'indietronica', 'modern rock', 'shimmer pop']\n",
      "UNK UNK tried to know its and guitars and instead of a UNK UNK we youll quite familiar with each other\n",
      "---\n",
      "['aussietronica', 'shimmer pop', 'vapor soul']\n",
      "the world was was b and his tracks are happy to come out relationship as not as well as among others\n",
      "---\n",
      "['indie r&b']\n",
      "the fact that weve been going through some of the project artist and waves in the time or above\n",
      "---\n",
      "['conscious hip hop', 'gangster rap', 'hip hop', 'pop rap', 'rap', 'southern hip hop', 'trap music', 'underground hip hop']\n",
      "at you such called near you you pull off with a three and that you featured on his previous tour\n",
      "---\n",
      "['alternative hip hop', 'escape room', 'hip hop', 'southern hip hop', 'underground hip hop']\n",
      "the UNK UNK UNK has been described as a collaboration with dream pop UNK and and here for free\n",
      "---\n",
      "['gangster rap', 'hip hop', 'pop rap', 'rap']\n",
      "UNK is the UNK UNK UNK look from the UNK and taking us into UNK UNK UNK as good time\n",
      "---\n",
      "['indie electro-pop', 'indie poptimism', 'indie psych-rock', 'vapor soul']\n",
      "UNK will first blend of alt pop songs and recorded in the fact of its taken from their first single since\n",
      "---\n",
      "['indie r&b']\n",
      "what not to make you feels like a great in addition to this is work in the uk artist reminds us with these uk\n",
      "---\n",
      "['indie electro-pop', 'indie poptimism', 'indie psych-rock']\n",
      "her latest UNK is part of a UNK at the UNK signed to UNK couple UNK ago\n",
      "---\n",
      "['electropop', 'indie electro-pop', 'indie poptimism', 'indie r&b', 'vapor soul']\n",
      "the song song here to have something different to the melody at the things in such a addition into something perfectly\n",
      "---\n",
      "['alternative dance', 'electropop', 'indie poptimism', 'indietronica', 'new rave', 'nu disco']\n",
      "the way festival in UNK and with a UNK of synths that shes made for a guy\n",
      "---\n",
      "['underground hip hop']\n",
      "we get familiar with a new american dream and catch him as the fell in music in listening to me\n",
      "---\n",
      "['vapor soul']\n",
      "with the song which grew up in the brooklyn based producer artist has created anna of the electronic fell in career\n",
      "---\n",
      "['soul']\n",
      "there is a bit of nothing but i listen to the record that UNK on beat records\n",
      "---\n",
      "['indie poptimism', 'vapor pop', 'vapor soul']\n",
      "the UNK set to set to your head UNK UNK UNK UNK UNK UNK UNK UNK UNK for in UNK months with the release of music and coming march\n",
      "---\n",
      "['indie poptimism', 'indietronica', 'metropopolis', 'shimmer pop']\n",
      "once hits right at UNK UNK but its a UNK and electro house beat for a vocals of dreamy synths and electric synths\n",
      "---\n",
      "['modern rock', 'neo-psychedelic']\n",
      "UNK UNK UNK UNK UNK is back in electronic music that i way back to up with a smile and keep your musical success\n",
      "---\n",
      "['deep house', 'deep tropical house', 'tropical house']\n",
      "a kind of UNK seems to finally caught up with the kind of music that more like the kind his previous age\n",
      "---\n",
      "['edm', 'electro house', 'electronic trap']\n",
      "the pop duo artist is made up and pop music is made with UNK right vibes its hits\n",
      "---\n",
      "['indie pop', 'indie poptimism', 'indietronica', 'la indie', 'modern alternative rock', 'modern rock']\n",
      "the song below along with the version of live in the late UNK UNK might with those live instrumentation\n",
      "---\n",
      "['alternative hip hop', 'big beat', 'electronic', 'hip hop', 'ninja', 'nu jazz', 'trip hop']\n",
      "its its a fan of new music of artist artist and one of the music ive been a couple of artists who have themselves\n",
      "---\n",
      "['chamber psych']\n",
      "im into the world with nothing in a first single to come ahead and its a store for its release\n",
      "---\n",
      "['alternative hip hop', 'conscious hip hop', 'hip hop', 'pop rap']\n",
      "to north american tour as well as a UNK has made up with hard hitting and and pick up some hype machine\n",
      "---\n",
      "['funk', 'indie jazz', 'indie r&b', 'neo soul', 'soul']\n",
      "the new UNK is to UNK that with a following on UNK UNK and you can create\n",
      "---\n",
      "['dirty south rap', 'gangster rap', 'hip hop', 'pop rap', 'rap', 'southern hip hop', 'trap music']\n",
      "UNK ep been working with with at people artist s fans\n",
      "---\n",
      "['house', 'pop', 'tropical house']\n",
      "now only a official remix grew up in the name for according to it remix\n",
      "---\n",
      "['chillwave', 'indie psych-rock', 'indietronica', 'shimmer pop']\n",
      "UNK is UNK UNK feels is a more in the type of music and we were need to fall in hype machine\n",
      "---\n",
      "['hip hop', 'pop', 'pop rap', 'rap']\n",
      "sounds like like to feature on the feel good vibes from kanye which is something to addition to her upcoming release\n",
      "---\n",
      "['brooklyn indie', 'chillwave', 'electropop', 'indie electro-pop', 'indie poptimism', 'indie psych-rock', 'indietronica', 'metropopolis', 'shimmer pop', 'vapor soul']\n",
      "UNK UNK UNK UNK UNK UNK artist returns with alt r b version\n",
      "---\n",
      "['alternative dance', 'big beat', 'downtempo', 'electronic', 'electropop', 'new rave', 'trip hop']\n",
      "but the UNK was but at it at a sold UNK UNK UNK at the show at start\n",
      "---\n",
      "['ninja', 'nu jazz']\n",
      "in la based artist artist artist with a smooth and and features artist from his artist\n",
      "---\n",
      "['chillwave', 'shimmer pop']\n",
      "like a UNK UNK are getting into a kind of music time then are before close to this ride\n",
      "---\n",
      "['chamber pop', 'folk-pop', 'indie folk', 'indie pop', 'new americana', 'stomp and holler']\n",
      "regardless of the UNK who are recorded in many the the doesnt go out on the late whatever\n",
      "---\n",
      "['funk', 'indie jazz', 'indie r&b', 'neo soul', 'soul']\n",
      "this has really nice to hear of fellow danish producer artist has up in our interview with him on recent mixtape\n",
      "---\n",
      "['chamber pop', 'indie anthem-folk', 'indie folk', 'indie r&b', 'vapor soul']\n",
      "a slice rising electro pop trio artist continues to have you in a a bass line and all once love with the single once again\n",
      "---\n",
      "['pop']\n",
      "this are a place to feature on this track that we dont expect from our stuff\n",
      "---\n",
      "['australian dance']\n",
      "the follow up to UNK UNK UNK UNK along with an brand new remix from hayden james\n",
      "---\n",
      "['deep tropical house']\n",
      "up with the release of artist will be him at the serious himself and himself in that its one instrumentation\n",
      "---\n",
      "['art pop', 'chamber psych']\n",
      "the this is another taste of their debut single and taken from the start of the same name\n",
      "---\n",
      "['dance pop', 'hip hop', 'hip pop', 'neo soul', 'pop', 'pop rap', 'r&b', 'rap', 'southern hip hop', 'urban contemporary']\n",
      "UNK UNK has a sound so much as being able to come out pop shows\n"
     ]
    }
   ],
   "source": [
    "gens, zs, conditions = generate.generate(vae, input_side, output_side, pairs, dataset, EMBED_SIZE, random_state, DEVICE, num_sample=n_samples, temp=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  'theres a new sounds at the there with a emotive and back in addition to each audience'),\n",
       " (1,\n",
       "  'not sure youve come of of UNK seems to released a couple of UNK earlier this years collaboration'),\n",
       " (2,\n",
       "  'now theyve released a five piece of us and their music vocals for a dream pop piece at music below'),\n",
       " (3,\n",
       "  'electro UNK UNK UNK according to the track itself into a different emotions and available on this weekend'),\n",
       " (4,\n",
       "  'UNK UNK UNK UNK UNK used to look at UNK the old and based in start of this coming today'),\n",
       " (5,\n",
       "  'felt like this image courtesy of artist and its sort of feels like track right'),\n",
       " (6,\n",
       "  'he started with the remix of artists such as he has made in an electronic songs that could made the top on the songs'),\n",
       " (7,\n",
       "  'producer artist sounds in say by these two track and check out than most recently'),\n",
       " (8,\n",
       "  'artist continues to is taken from the album but theyve returned to a album that that sounded like find effect'),\n",
       " (9, 'this is off from a few days ago and because of their this coming games'),\n",
       " (10,\n",
       "  'around the UNK below doesnt look the UNK and listen to UNK UNK or reminiscent of the his voice'),\n",
       " (11,\n",
       "  'UNK just dropped an official remix of UNK track since we yet to moving away from the courtesy of electronic music together'),\n",
       " (12,\n",
       "  'from the image courtesy of guy who continues to obsessed with everyone from different from the as i had to comes from this scene'),\n",
       " (13,\n",
       "  'until the UNK took your day and thought it would be featured on the last UNK cover'),\n",
       " (14,\n",
       "  'with only a bit of production and and youre a fan of the above to add a live instrumentation'),\n",
       " (15,\n",
       "  'back in UNK and with the new single from UNK who just released a follow up for the next album on coming soon'),\n",
       " (16,\n",
       "  'by artist recently released the official single from the UNK below and be sure to drop their upcoming album on late october'),\n",
       " (17,\n",
       "  'image courtesy of artist as described as you above and follow official it for last time'),\n",
       " (18,\n",
       "  'artist is the a long way that way back in the kind after building on the new production'),\n",
       " (19,\n",
       "  'and behind much less from artist who lead able thanks to a less spot on pop instrumental'),\n",
       " (20,\n",
       "  'to look at the rest of the UNK is going i allows you to listen to to enjoy'),\n",
       " (21,\n",
       "  'the track is itself in its a piece a groove pace with its its guitars and perfectly backing vocals'),\n",
       " (22,\n",
       "  'in UNK UNK UNK UNK UNK has returned with the likes of UNK material since UNK who returns with the first single after a few years eps'),\n",
       " (23,\n",
       "  'away from the three piece impressed us with duos first single UNK which it features to james set'),\n",
       " (24,\n",
       "  'some UNK is different from the year the to to some reason why i feel every single'),\n",
       " (25, 'UNK UNK at neo soul electronic and wont have signed to facebook june'),\n",
       " (26,\n",
       "  'the la the trio artist have released so many of us with a on tour and stay with your free'),\n",
       " (27,\n",
       "  'lost in the back in the soulful back and a slow bit of his previous seconds'),\n",
       " (28,\n",
       "  'i was working on a debut album but you and find out at the and that but with you with any just as just as as more'),\n",
       " (29,\n",
       "  'UNK came out with a big ovo sound artist and the kind of ovo sound kicks off'),\n",
       " (30,\n",
       "  'anna of UNK UNK UNK UNK for a slew of new years of the north returns with the new music which was anna of the last few years'),\n",
       " (31,\n",
       "  'the ability to UNK UNK and that UNK is the seem to put them into one minute reason they made going on me set'),\n",
       " (32,\n",
       "  'but i have love gone and when i fell in love but thats worth my love lately'),\n",
       " (33,\n",
       "  'the berlin french producer artist has to be UNK UNK UNK UNK i fell i love style'),\n",
       " (34,\n",
       "  'were re away from the likes of artist and stay ready for the over over before well before their third full length'),\n",
       " (35,\n",
       "  'my official anna of the UNK UNK UNK up on its an ethereal electro UNK vibe'),\n",
       " (36,\n",
       "  'you you as a part of UNK who and you at the UNK UNK UNK started working on your previous work'),\n",
       " (37,\n",
       "  'they are playing out of the record and return to the show at the record that will take the back in side of the remix from there on march'),\n",
       " (38,\n",
       "  'have an almost as one of the UNK and its almost an up and warm and melodic focus on their vocal vibe'),\n",
       " (39,\n",
       "  'i have heard this song with the one of my friend and and in and and electric feel doing it take space'),\n",
       " (40,\n",
       "  'i was inspired by some of the UNK that its been remixed by UNK years ago since UNK explains'),\n",
       " (41,\n",
       "  'the the duo artist have taken from his forthcoming new ep which is part of my very lost in my love'),\n",
       " (42,\n",
       "  'UNK UNK UNK UNK UNK UNK is the friends and im at the room for a whole or chance'),\n",
       " (43,\n",
       "  'taken from artist and weve come since the release of UNK UNK UNK UNK will holding onto your no party'),\n",
       " (44,\n",
       "  'we always back to a new door cinema club which fit for a track and i feel and below along with some electro beats'),\n",
       " (45,\n",
       "  'UNK UNK UNK UNK is away with the remix of UNK which were in the festival this weekend'),\n",
       " (46,\n",
       "  'UNK is UNK nothing and talking about the UNK UNK UNK to the music in a start of a UNK and direction that real relationship'),\n",
       " (47,\n",
       "  'UNK is very much UNK UNK UNK UNK UNK and UNK UNK UNK UNK UNK UNK UNK UNK in a very guitars like the sound very much of the instrumentation and produced by james machine'),\n",
       " (48,\n",
       "  'feels like youre able to know there is the first time youre to make it right around your connect'),\n",
       " (49,\n",
       "  'artist had a part of the UNK are combined of a UNK with you up in a bit for you while'),\n",
       " (50,\n",
       "  'the first tracks to find you with the official remixes of the official remixes and and called rework below and grab it below and today'),\n",
       " (51,\n",
       "  'the track was produced too many artists world as while youve yet to keep two singles fast'),\n",
       " (52,\n",
       "  'you can also really clear that there are also over the kind of pop music from last UNK but it can also whats to come over the last we love that yet'),\n",
       " (53,\n",
       "  'UNK UNK UNK UNK and UNK is an UNK of UNK and a UNK that has been working of the night since UNK release'),\n",
       " (54,\n",
       "  'this UNK UNK were working on the dark indie rock sound that is taking listeners into a on packed with soft and light as well featuring the front man himself'),\n",
       " (55, 'so much of the me as i saying that this UNK and open up of summer'),\n",
       " (56, 'the the sounds of UNK is just with a UNK UNK with UNK songs style'),\n",
       " (57,\n",
       "  'this UNK UNK is heading to select festival in october in which which returns with the release the york month'),\n",
       " (58,\n",
       "  'it feels right here in sultry voice that serve by a before and were both in their ear'),\n",
       " (59,\n",
       "  'i was able to get right into the synths and it with the the right as the right sounds'),\n",
       " (60,\n",
       "  'UNK UNK tried to know its and guitars and instead of a UNK UNK we youll quite familiar with each other'),\n",
       " (61,\n",
       "  'the world was was b and his tracks are happy to come out relationship as not as well as among others'),\n",
       " (62,\n",
       "  'the fact that weve been going through some of the project artist and waves in the time or above'),\n",
       " (63,\n",
       "  'at you such called near you you pull off with a three and that you featured on his previous tour'),\n",
       " (64,\n",
       "  'the UNK UNK UNK has been described as a collaboration with dream pop UNK and and here for free'),\n",
       " (65,\n",
       "  'UNK is the UNK UNK UNK look from the UNK and taking us into UNK UNK UNK as good time'),\n",
       " (66,\n",
       "  'UNK will first blend of alt pop songs and recorded in the fact of its taken from their first single since'),\n",
       " (67,\n",
       "  'what not to make you feels like a great in addition to this is work in the uk artist reminds us with these uk'),\n",
       " (68,\n",
       "  'her latest UNK is part of a UNK at the UNK signed to UNK couple UNK ago'),\n",
       " (69,\n",
       "  'the song song here to have something different to the melody at the things in such a addition into something perfectly'),\n",
       " (70,\n",
       "  'the way festival in UNK and with a UNK of synths that shes made for a guy'),\n",
       " (71,\n",
       "  'we get familiar with a new american dream and catch him as the fell in music in listening to me'),\n",
       " (72,\n",
       "  'with the song which grew up in the brooklyn based producer artist has created anna of the electronic fell in career'),\n",
       " (73,\n",
       "  'there is a bit of nothing but i listen to the record that UNK on beat records'),\n",
       " (74,\n",
       "  'the UNK set to set to your head UNK UNK UNK UNK UNK UNK UNK UNK UNK for in UNK months with the release of music and coming march'),\n",
       " (75,\n",
       "  'once hits right at UNK UNK but its a UNK and electro house beat for a vocals of dreamy synths and electric synths'),\n",
       " (76,\n",
       "  'UNK UNK UNK UNK UNK is back in electronic music that i way back to up with a smile and keep your musical success'),\n",
       " (77,\n",
       "  'a kind of UNK seems to finally caught up with the kind of music that more like the kind his previous age'),\n",
       " (78,\n",
       "  'the pop duo artist is made up and pop music is made with UNK right vibes its hits'),\n",
       " (79,\n",
       "  'the song below along with the version of live in the late UNK UNK might with those live instrumentation'),\n",
       " (80,\n",
       "  'its its a fan of new music of artist artist and one of the music ive been a couple of artists who have themselves'),\n",
       " (81,\n",
       "  'im into the world with nothing in a first single to come ahead and its a store for its release'),\n",
       " (82,\n",
       "  'to north american tour as well as a UNK has made up with hard hitting and and pick up some hype machine'),\n",
       " (83,\n",
       "  'the new UNK is to UNK that with a following on UNK UNK and you can create'),\n",
       " (84, 'UNK ep been working with with at people artist s fans'),\n",
       " (85,\n",
       "  'now only a official remix grew up in the name for according to it remix'),\n",
       " (86,\n",
       "  'UNK is UNK UNK feels is a more in the type of music and we were need to fall in hype machine'),\n",
       " (87,\n",
       "  'sounds like like to feature on the feel good vibes from kanye which is something to addition to her upcoming release'),\n",
       " (88, 'UNK UNK UNK UNK UNK UNK artist returns with alt r b version'),\n",
       " (89, 'but the UNK was but at it at a sold UNK UNK UNK at the show at start'),\n",
       " (90,\n",
       "  'in la based artist artist artist with a smooth and and features artist from his artist'),\n",
       " (91,\n",
       "  'like a UNK UNK are getting into a kind of music time then are before close to this ride'),\n",
       " (92,\n",
       "  'regardless of the UNK who are recorded in many the the doesnt go out on the late whatever'),\n",
       " (93,\n",
       "  'this has really nice to hear of fellow danish producer artist has up in our interview with him on recent mixtape'),\n",
       " (94,\n",
       "  'a slice rising electro pop trio artist continues to have you in a a bass line and all once love with the single once again'),\n",
       " (95,\n",
       "  'this are a place to feature on this track that we dont expect from our stuff'),\n",
       " (96,\n",
       "  'the follow up to UNK UNK UNK UNK along with an brand new remix from hayden james'),\n",
       " (97,\n",
       "  'up with the release of artist will be him at the serious himself and himself in that its one instrumentation'),\n",
       " (98,\n",
       "  'the this is another taste of their debut single and taken from the start of the same name'),\n",
       " (99, 'UNK UNK has a sound so much as being able to come out pop shows')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(range(len(gens)), gens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_embed(z, condition):\n",
    "    if condition.dim() == 1:\n",
    "        condition = condition.unsqueeze(0)\n",
    "    squashed_condition = vae.decoder.c2h(condition)\n",
    "    return torch.cat([z, squashed_condition], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latent = 128\n",
    "from pytorchtextvae.datasets import EOS_token\n",
    "\n",
    "def generate(condition, gan=None, z=None, max_sample=False, truncated_sample=True, temp=temp):\n",
    "    with torch.no_grad():\n",
    "        if gan is None:\n",
    "            z_prime = z\n",
    "        else:\n",
    "            gan.eval()\n",
    "            z = torch.randn(1, n_latent).to(DEVICE)\n",
    "            decode_embed = to_embed(z, condition).to(DEVICE)\n",
    "            z_prime = gan.G(decode_embed)\n",
    "\n",
    "        generated = vae.decoder.generate_with_embed(z_prime, 50, temp, DEVICE, max_sample=max_sample, trunc_sample=truncated_sample)\n",
    "        generated_str = model.float_word_tensor_to_string(output_side, generated)\n",
    "\n",
    "        EOS_str = f' {output_side.index_to_word(torch.LongTensor([EOS_token]))} '\n",
    "\n",
    "        if generated_str.endswith(EOS_str):\n",
    "            generated_str = generated_str[:-5]\n",
    "\n",
    "        # flip it back\n",
    "        return generated_str[::-1], z, z_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 'Banned' approach\n",
    "\n",
    "label a sample as -1 (==\"bad\") if it contains a banned word; label as 1 otherwise (==\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new_labels = np.array([(1, -1), (10, -1)])\n",
    "\n",
    "banned = ['below']\n",
    "labels = np.ones(n_samples, dtype=int)\n",
    "gens_lose = list(set([i for b in banned for i in np.where([b in g.split() for g in gens])[0]]))\n",
    "labels[gens_lose] = -1\n",
    "zs_keep = zs\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('below', 3)],\n",
       " [('the', 94),\n",
       "  ('UNK', 82),\n",
       "  ('of', 67),\n",
       "  ('a', 66),\n",
       "  ('and', 59),\n",
       "  ('to', 55),\n",
       "  ('is', 29),\n",
       "  ('as', 26),\n",
       "  ('artist', 26),\n",
       "  ('with', 26),\n",
       "  ('from', 24),\n",
       "  ('it', 21),\n",
       "  ('this', 21),\n",
       "  ('track', 20),\n",
       "  ('on', 20),\n",
       "  ('up', 19),\n",
       "  ('one', 18),\n",
       "  ('be', 15),\n",
       "  ('in', 14),\n",
       "  ('has', 14),\n",
       "  ('at', 11),\n",
       "  ('i', 10),\n",
       "  ('been', 10),\n",
       "  ('trying', 10),\n",
       "  ('that', 10)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c1 = Counter([word for gen in gens for word in gen.split()])\n",
    "[(b, c1[b]) for b in banned], c1.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 'Realism' approach\n",
    "\n",
    "label a sample as 1 (==\"good\") if it came from the training data; label as -1 (==\"bad\") if it came from a random Gaussian `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('newcomer artist released his debut single last week and its already gaining major attention and a following that is demanding more after fill EOS ',\n",
       " ['vapor soul'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorchtextvae import model\n",
    "\n",
    "input, target, condition = model.random_training_set(pairs, input_side, output_side, random_state, DEVICE)\n",
    "model.long_word_tensor_to_string(input_side, input), dataset.decode_genres(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), torch.Size([24, 333336]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = 1.0\n",
    "\n",
    "m, l, z, decoded = vae(input, target, condition, DEVICE, temperature)\n",
    "\n",
    "z.size(), decoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artist released his debut single and is just released and more than a ago and that that get your attention'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(condition, z=z, max_sample=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**TODO:** shouldn't generate with max sampling always return the same sample?\n",
    "\n",
    "Even though the encoding is imperfect, we will still consider these `z`s as \"realistic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "real_z = []\n",
    "real_gens = []\n",
    "for i in range(int(n_samples/2)):\n",
    "    input, target, condition = model.random_training_set(pairs, input_side, output_side, random_state, DEVICE)\n",
    "    with torch.no_grad():\n",
    "        _, _, z, _ = vae(input, target, condition, DEVICE, temperature)\n",
    "        real_z.append(z)\n",
    "        real_gens.append(generate(condition, z=z, max_sample=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UNK', 106),\n",
       " ('and', 53),\n",
       " ('the', 50),\n",
       " ('a', 30),\n",
       " ('of', 30),\n",
       " ('on', 28),\n",
       " ('to', 24),\n",
       " ('in', 15),\n",
       " ('is', 15),\n",
       " ('i', 14),\n",
       " ('with', 14),\n",
       " ('w', 13),\n",
       " ('even', 11),\n",
       " ('that', 10),\n",
       " ('for', 10),\n",
       " ('who', 10),\n",
       " ('this', 9),\n",
       " ('always', 9),\n",
       " ('it', 9),\n",
       " ('tour', 8),\n",
       " ('into', 8),\n",
       " ('be', 8),\n",
       " ('but', 8),\n",
       " ('you', 7),\n",
       " ('new', 7)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c1 = Counter([word for gen in real_gens for word in gen.split()])\n",
    "c1.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# up until now, `zs` held random zs - now concat with real zs\n",
    "zs = torch.cat((torch.stack(real_z).squeeze(), torch.stack(zs[:int(n_samples/2)]).squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.ones(n_samples, dtype=int)\n",
    "labels[range(len(real_z), len(zs))] = -1\n",
    "embeds = zs\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Realism + Readability\n",
    "in addition to the realism discriminator, add readability as a conditioning attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92.8, 116.15]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/shivam5992/textstat/issues/43\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "# Score \t Difficulty\n",
    "# 90-100 \t Very Easy\n",
    "# 80-89 \t Easy\n",
    "# 70-79 \t Fairly Easy\n",
    "# 60-69 \t Standard\n",
    "# 50-59 \t Fairly Difficult\n",
    "# 30-49 \t Difficult\n",
    "# 0-29 \t Very Confusing\n",
    "\n",
    "[textstat.flesch_reading_ease(sent) for sent in [\"This is a sentence\", \"To be or not to be\", ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one of those who dont want to the one of their own who are their own in their own music',\n",
       "  '4th and 5th grade'),\n",
       " ('the remix the production from the original and it sounds like a it with the bass and that sounds like it sounds like it from the chorus',\n",
       "  '6th and 7th grade'),\n",
       " ('we been waiting for the last year while while its soon as waiting for little while we we as we as it',\n",
       "  '12th and 13th grade'),\n",
       " ('one of those who dont want to the one of their own who are their own in their own music',\n",
       "  '4th and 5th grade'),\n",
       " ('we trying to get into an artist is at the one of the and in the song that a part of the is in the trying to into it into a song to artist is just into part into that that or else',\n",
       "  '14th and 15th grade'),\n",
       " ('if youre on the first version of you of know if you hear the version of this is that is to a song',\n",
       "  '1th and 2th grade'),\n",
       " ('we been waiting for the last year while while its soon as waiting for little while we we as we as it',\n",
       "  '12th and 13th grade'),\n",
       " ('theres everything about the song or what it it just everything about it comes from the lyrics and you should be comes to our lately',\n",
       "  '11th and 12th grade'),\n",
       " ('its the most best and to think its the best and think its the good moment',\n",
       "  '3th and 4th grade'),\n",
       " ('look look for the song below and look for yourself in',\n",
       "  '4th and 5th grade')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(' '.join(gen.replace('UNK', '').split()), textstat.text_standard(' '.join(gen.replace('UNK', '').split()))) for gen in np.array(real_gens)[random_state.choice(len(real_gens), 10)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topical\n",
    "Prefer certain topics to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchtextvae.datasets as ds\n",
    "\n",
    "def tokenize(line):\n",
    "    l = line.strip().lstrip().rstrip()\n",
    "    l = ds.normalize_string(l)\n",
    "    return l.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['after',\n",
       "  'releasing',\n",
       "  'this',\n",
       "  'single',\n",
       "  'meg',\n",
       "  'mac',\n",
       "  'is',\n",
       "  'set',\n",
       "  'to',\n",
       "  'tour',\n",
       "  'around',\n",
       "  'australia',\n",
       "  'in',\n",
       "  'september',\n",
       "  'then',\n",
       "  'returning',\n",
       "  'to',\n",
       "  'the',\n",
       "  'usa',\n",
       "  'to',\n",
       "  'continue',\n",
       "  'to',\n",
       "  'work',\n",
       "  'on',\n",
       "  'her',\n",
       "  'full',\n",
       "  'length',\n",
       "  'debut',\n",
       "  'album'],\n",
       " ['sergei',\n",
       "  'wicking',\n",
       "  'was',\n",
       "  'surrounded',\n",
       "  'by',\n",
       "  'music',\n",
       "  'since',\n",
       "  'a',\n",
       "  'young',\n",
       "  'age',\n",
       "  'and',\n",
       "  'recently',\n",
       "  'has',\n",
       "  'taken',\n",
       "  'to',\n",
       "  'writing',\n",
       "  'his',\n",
       "  'thoughts',\n",
       "  'and',\n",
       "  'feelings',\n",
       "  'about',\n",
       "  'new',\n",
       "  'artists'],\n",
       " ['the',\n",
       "  'track',\n",
       "  'is',\n",
       "  'really',\n",
       "  'melodic',\n",
       "  'and',\n",
       "  'with',\n",
       "  'a',\n",
       "  'top',\n",
       "  'notch',\n",
       "  'top',\n",
       "  'line',\n",
       "  'to',\n",
       "  'boot',\n",
       "  'dont',\n",
       "  'leave',\n",
       "  'is',\n",
       "  'the',\n",
       "  'threesomes',\n",
       "  'most',\n",
       "  'impressive',\n",
       "  'production',\n",
       "  'yet']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_examples = 3\n",
    "\n",
    "sents = [pair[0] for pair in pairs]\n",
    "texts = [tokenize(sentence) for sentence in sents]\n",
    "texts[:n_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['releasing',\n",
       "  'single',\n",
       "  'meg',\n",
       "  'mac',\n",
       "  'set',\n",
       "  'tour',\n",
       "  'around',\n",
       "  'australia',\n",
       "  'september',\n",
       "  'returning',\n",
       "  'usa',\n",
       "  'continue',\n",
       "  'work',\n",
       "  'full',\n",
       "  'length',\n",
       "  'debut',\n",
       "  'album'],\n",
       " ['sergei',\n",
       "  'wicking',\n",
       "  'surrounded',\n",
       "  'music',\n",
       "  'since',\n",
       "  'young',\n",
       "  'age',\n",
       "  'recently',\n",
       "  'taken',\n",
       "  'writing',\n",
       "  'thoughts',\n",
       "  'feelings',\n",
       "  'new',\n",
       "  'artists'],\n",
       " ['track',\n",
       "  'really',\n",
       "  'melodic',\n",
       "  'top',\n",
       "  'notch',\n",
       "  'top',\n",
       "  'line',\n",
       "  'boot',\n",
       "  'leave',\n",
       "  'threesomes',\n",
       "  'impressive',\n",
       "  'production',\n",
       "  'yet']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "stoplist = [ds.normalize_string(word) for word in stopwords.words('english')]\n",
    "fillerlist = ['author', 'song_title', 'artist', 'sitename']\n",
    "\n",
    "texts = [[word for word in text if word not in stoplist and word not in fillerlist] for text in texts]\n",
    "texts[:n_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "dictionary = Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"one\" + 0.012*\"like\" + 0.012*\"song\" + 0.011*\"music\" + 0.010*\"time\" + 0.006*\"get\" + 0.006*\"us\" + 0.006*\"really\" + 0.006*\"track\" + 0.006*\"way\"'),\n",
       " (1,\n",
       "  '0.016*\"remix\" + 0.008*\"soundcloud\" + 0.008*\"music\" + 0.006*\"free\" + 0.005*\"check\" + 0.005*\"stream\" + 0.005*\"get\" + 0.005*\"track\" + 0.005*\"video\" + 0.004*\"facebook\"'),\n",
       " (2,\n",
       "  '0.027*\"new\" + 0.021*\"album\" + 0.019*\"single\" + 0.014*\"release\" + 0.014*\"ep\" + 0.012*\"debut\" + 0.011*\"released\" + 0.011*\"track\" + 0.010*\"year\" + 0.010*\"first\"'),\n",
       " (3,\n",
       "  '0.017*\"track\" + 0.014*\"pop\" + 0.012*\"vocals\" + 0.007*\"sound\" + 0.006*\"electronic\" + 0.006*\"production\" + 0.006*\"dance\" + 0.005*\"song\" + 0.005*\"vocal\" + 0.005*\"remix\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "n_topics = 4\n",
    "passes = 20 # number of passes through documents\n",
    "iterations = 400\n",
    "eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(corpus, id2word=dictionary, num_topics=n_topics, iterations=iterations, passes=passes, eval_every=eval_every)\n",
    "#lda = LdaModel(corpus, id2word=dictionary, num_topics=n_topics)\n",
    "lda.print_topics(n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 0.75774455) after releasing this single meg mac is set to tour around australia in september then returning to the usa to continue to work on her full length debut album\n",
      "(0, 0.6420067) sergei wicking was surrounded by music since a young age and recently has taken to writing his thoughts and feelings about new artists\n",
      "(3, 0.5600023) the track is really melodic and with a top notch top line to boot dont leave is the threesomes most impressive production yet\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "for i in range(n_examples):\n",
    "    print(max(lda[corpus[i]],key=itemgetter(1)), ds.normalize_string(sents[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el252351404985145292807646282134\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el252351404985145292807646282134_data = {\"mdsDat\": {\"x\": [-0.07692405905239924, -0.10750651666479105, -0.151251602435944, 0.3356821781531343], \"y\": [-0.08286202580491597, -0.2292031021891245, 0.27882561817288926, 0.03323950982115113], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [29.261842727661133, 29.184537887573242, 26.3366641998291, 15.216954231262207]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"Freq\": [7074.0, 12723.0, 7620.0, 4830.0, 4704.0, 4977.0, 5432.0, 4267.0, 4611.0, 3692.0, 3895.0, 4218.0, 5151.0, 2423.0, 1637.0, 2315.0, 6995.0, 2227.0, 3220.0, 2171.0, 1890.0, 2962.0, 3107.0, 1790.0, 3373.0, 1868.0, 1869.0, 2591.0, 7579.0, 2435.0, 3894.245849609375, 1868.4976806640625, 1783.279541015625, 1699.1728515625, 1621.5936279296875, 1538.023193359375, 1386.4739990234375, 1358.4766845703125, 1343.881103515625, 1343.9852294921875, 1306.3983154296875, 1316.4488525390625, 1314.4803466796875, 1245.6019287109375, 1198.87548828125, 1209.2235107421875, 1194.7161865234375, 1091.03955078125, 1097.781005859375, 1080.4427490234375, 1060.096435546875, 1041.45556640625, 987.87744140625, 925.3843994140625, 839.76708984375, 813.4092407226562, 769.8744506835938, 723.38916015625, 703.0361328125, 733.1741333007812, 1435.8797607421875, 2204.333251953125, 2005.3509521484375, 5265.7314453125, 4589.8359375, 1809.839599609375, 2114.2314453125, 4423.251953125, 1759.6702880859375, 2271.893798828125, 4044.189453125, 1738.2003173828125, 1443.1260986328125, 2217.29541015625, 2112.380615234375, 1495.4556884765625, 1660.2742919921875, 2153.317626953125, 1653.8448486328125, 1908.2840576171875, 1463.18359375, 1359.3414306640625, 4610.94921875, 2314.94482421875, 2226.49755859375, 2171.175537109375, 1867.719482421875, 1709.6591796875, 1694.2747802734375, 1636.8739013671875, 1456.9046630859375, 1466.1343994140625, 1329.5584716796875, 1293.3717041015625, 1238.67724609375, 1188.8927001953125, 1128.478515625, 1050.8048095703125, 1037.9259033203125, 1029.817138671875, 1035.1302490234375, 975.4239501953125, 951.49267578125, 950.5958862304688, 924.3784790039062, 916.6253051757812, 906.9049682617188, 904.6000366210938, 846.9591674804688, 814.6417846679688, 792.5491333007812, 734.44921875, 5263.9873046875, 1824.9451904296875, 2632.06591796875, 1452.245849609375, 6633.3134765625, 1558.9962158203125, 1366.1458740234375, 1829.774169921875, 2041.9981689453125, 1130.33984375, 1709.7637939453125, 1390.691650390625, 1139.0997314453125, 1131.18408203125, 7073.869140625, 4829.40869140625, 4703.9990234375, 4267.00341796875, 3691.57080078125, 2422.30859375, 1889.8653564453125, 1789.3831787109375, 1653.0589599609375, 1558.728515625, 1428.9554443359375, 1311.24169921875, 1261.5369873046875, 1222.9068603515625, 1156.883056640625, 1090.5406494140625, 1088.6961669921875, 1071.21337890625, 1033.365478515625, 991.0054931640625, 975.523193359375, 957.6640625, 961.0037231445312, 937.460205078125, 930.6932983398438, 875.045654296875, 828.1328735351562, 840.7015380859375, 809.7460327148438, 751.0139770507812, 6488.70263671875, 9424.255859375, 3512.43505859375, 2346.48876953125, 3497.5078125, 2332.312255859375, 1941.9217529296875, 1993.2027587890625, 1337.679931640625, 3685.23681640625, 1625.07470703125, 1468.5406494140625, 1629.5972900390625, 1203.3682861328125, 1239.7105712890625, 1437.5767822265625, 1636.5745849609375, 1160.6031494140625, 1041.7919921875, 879.0922241210938, 849.6235961914062, 790.3778076171875, 808.9381713867188, 703.8373413085938, 639.6006469726562, 597.7335815429688, 556.6277465820312, 548.655517578125, 541.603759765625, 533.2763061523438, 525.9364624023438, 497.4949645996094, 466.5014343261719, 401.77899169921875, 404.7742004394531, 395.21234130859375, 399.444580078125, 399.19110107421875, 389.03424072265625, 392.54290771484375, 377.5636901855469, 375.8669738769531, 356.3005676269531, 350.8573913574219, 351.30169677734375, 356.4828796386719, 3147.632080078125, 1046.8431396484375, 513.2551879882812, 906.760498046875, 813.8649291992188, 661.6982421875, 549.3843994140625, 707.5380249023438, 864.3502197265625, 736.0465698242188, 591.3990478515625, 1508.5333251953125, 947.8965454101562, 798.1337280273438, 659.3106689453125, 729.4805908203125, 688.1428833007812, 925.4524536132812, 648.9584350585938, 695.8713989257812, 671.3474731445312, 569.8114624023438], \"Term\": [\"album\", \"new\", \"single\", \"release\", \"ep\", \"remix\", \"pop\", \"debut\", \"vocals\", \"released\", \"time\", \"year\", \"first\", \"based\", \"soundcloud\", \"electronic\", \"like\", \"production\", \"get\", \"dance\", \"tour\", \"producer\", \"last\", \"records\", \"sound\", \"vocal\", \"im\", \"via\", \"one\", \"really\", \"time\", \"im\", \"know\", \"going\", \"think\", \"would\", \"every\", \"want\", \"could\", \"things\", \"people\", \"say\", \"got\", \"heard\", \"ive\", \"never\", \"weve\", \"might\", \"though\", \"many\", \"lot\", \"pretty\", \"thing\", \"cant\", \"getting\", \"need\", \"moment\", \"said\", \"listening\", \"start\", \"life\", \"really\", \"something\", \"one\", \"like\", \"good\", \"way\", \"song\", \"much\", \"get\", \"music\", \"even\", \"world\", \"us\", \"love\", \"songs\", \"back\", \"track\", \"first\", \"new\", \"make\", \"feel\", \"vocals\", \"electronic\", \"production\", \"dance\", \"vocal\", \"guitar\", \"synth\", \"beat\", \"bass\", \"beats\", \"synths\", \"b\", \"r\", \"soul\", \"voice\", \"electro\", \"disco\", \"vibe\", \"style\", \"chorus\", \"melodies\", \"hop\", \"piano\", \"hip\", \"smooth\", \"deep\", \"melody\", \"catchy\", \"vibes\", \"slow\", \"pop\", \"house\", \"sound\", \"indie\", \"track\", \"original\", \"sounds\", \"remix\", \"song\", \"tune\", \"like\", \"new\", \"latest\", \"single\", \"album\", \"release\", \"ep\", \"debut\", \"released\", \"based\", \"tour\", \"records\", \"upcoming\", \"called\", \"london\", \"titled\", \"forthcoming\", \"uk\", \"1\", \"october\", \"due\", \"2015\", \"2014\", \"10\", \"festival\", \"month\", \"label\", \"lp\", \"dropped\", \"2\", \"september\", \"2016\", \"dates\", \"york\", \"single\", \"new\", \"year\", \"producer\", \"first\", \"last\", \"via\", \"duo\", \"today\", \"track\", \"latest\", \"years\", \"back\", \"week\", \"since\", \"music\", \"soundcloud\", \"free\", \"stream\", \"facebook\", \"download\", \"twitter\", \"let\", \"friends\", \"itunes\", \"order\", \"posted\", \"watch\", \"plays\", \"everyone\", \"enjoy\", \"spotify\", \"grab\", \"hype\", \"favourite\", \"million\", \"able\", \"white\", \"machine\", \"immediately\", \"matter\", \"creative\", \"pick\", \"media\", \"list\", \"recordings\", \"remix\", \"check\", \"pre\", \"video\", \"artists\", \"follow\", \"cover\", \"remixes\", \"listen\", \"sure\", \"version\", \"music\", \"get\", \"also\", \"already\", \"original\", \"make\", \"track\", \"via\", \"like\", \"one\", \"songs\"], \"Total\": [7074.0, 12723.0, 7620.0, 4830.0, 4704.0, 4977.0, 5432.0, 4267.0, 4611.0, 3692.0, 3895.0, 4218.0, 5151.0, 2423.0, 1637.0, 2315.0, 6995.0, 2227.0, 3220.0, 2171.0, 1890.0, 2962.0, 3107.0, 1790.0, 3373.0, 1868.0, 1869.0, 2591.0, 7579.0, 2435.0, 3895.015625, 1869.2515869140625, 1784.0361328125, 1699.9365234375, 1622.3818359375, 1538.7855224609375, 1387.2469482421875, 1359.2403564453125, 1344.642333984375, 1344.7535400390625, 1307.14892578125, 1317.2081298828125, 1315.26025390625, 1246.380859375, 1199.62841796875, 1209.98583984375, 1195.4947509765625, 1091.802490234375, 1098.5570068359375, 1081.2109375, 1060.8524169921875, 1042.2265625, 988.6377563476562, 926.1464233398438, 840.53515625, 814.1727294921875, 770.653076171875, 724.1620483398438, 703.8026733398438, 733.9757080078125, 1494.655029296875, 2435.261962890625, 2338.007080078125, 7579.11865234375, 6995.72607421875, 2188.53955078125, 2784.1220703125, 7515.7998046875, 2206.685302734375, 3220.3037109375, 7626.51513671875, 2331.32958984375, 1735.2138671875, 3641.877685546875, 3479.372314453125, 2349.81201171875, 4165.62744140625, 13397.3193359375, 5151.8720703125, 12723.4951171875, 2516.738037109375, 1942.593994140625, 4611.6982421875, 2315.724609375, 2227.257080078125, 2171.94482421875, 1868.465576171875, 1710.4053955078125, 1695.019775390625, 1637.630859375, 1457.6505126953125, 1466.8961181640625, 1330.2977294921875, 1294.1348876953125, 1239.4432373046875, 1189.662841796875, 1129.2303466796875, 1051.5611572265625, 1038.6832275390625, 1030.56982421875, 1035.8895263671875, 976.1739501953125, 952.2356567382812, 951.3611450195312, 925.123046875, 917.3804931640625, 907.6533203125, 905.3701171875, 847.70458984375, 815.3907470703125, 793.3056640625, 735.2072143554688, 5432.49853515625, 2147.8125, 3373.091796875, 1647.8765869140625, 13397.3193359375, 2289.0, 2049.53857421875, 4977.91845703125, 7515.7998046875, 1665.1512451171875, 6995.72607421875, 12723.4951171875, 2764.6923828125, 7620.40087890625, 7074.62353515625, 4830.1611328125, 4704.75390625, 4267.74853515625, 3692.31787109375, 2423.067138671875, 1890.61181640625, 1790.13037109375, 1653.80419921875, 1559.4853515625, 1429.7113037109375, 1311.990234375, 1262.276611328125, 1223.6689453125, 1157.6405029296875, 1091.27734375, 1089.44140625, 1071.9669189453125, 1034.1143798828125, 991.74609375, 976.2684326171875, 958.4130249023438, 961.7578125, 938.2069091796875, 931.4463500976562, 875.808837890625, 828.8734130859375, 841.4556884765625, 810.4862670898438, 751.7586669921875, 7620.40087890625, 12723.4951171875, 4218.10205078125, 2962.6142578125, 5151.8720703125, 3107.9345703125, 2591.366943359375, 2824.779296875, 1648.46533203125, 13397.3193359375, 2764.6923828125, 2423.769287109375, 4165.62744140625, 1572.946044921875, 1890.18505859375, 7626.51513671875, 1637.3173828125, 1161.3541259765625, 1042.5484619140625, 879.8268432617188, 850.3561401367188, 791.1148681640625, 809.7081298828125, 704.6051025390625, 640.3392944335938, 598.48095703125, 557.3798828125, 549.4044799804688, 542.3677368164062, 534.05615234375, 526.7093505859375, 498.2438049316406, 467.2451477050781, 402.5334777832031, 405.5437927246094, 395.96966552734375, 400.2126770019531, 399.9637756347656, 389.79095458984375, 393.3326416015625, 378.3366394042969, 376.6497497558594, 357.0523376464844, 351.60247802734375, 352.0617980957031, 357.2614440917969, 4977.91845703125, 1507.3201904296875, 628.5826416015625, 1672.340087890625, 1557.671875, 1083.2783203125, 763.7799682617188, 1259.5804443359375, 2079.119140625, 1503.713623046875, 941.14404296875, 7626.51513671875, 3220.3037109375, 2975.134033203125, 1540.29736328125, 2289.0, 2516.738037109375, 13397.3193359375, 2591.366943359375, 6995.72607421875, 7579.11865234375, 2349.81201171875], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.228700041770935, 1.2285000085830688, 1.2285000085830688, 1.2283999919891357, 1.2283999919891357, 1.2283999919891357, 1.2282999753952026, 1.2282999753952026, 1.2282999753952026, 1.2282999753952026, 1.2282999753952026, 1.2282999753952026, 1.2282999753952026, 1.2282999753952026, 1.2282999753952026, 1.2282999753952026, 1.2281999588012695, 1.2281999588012695, 1.2281999588012695, 1.2281999588012695, 1.2281999588012695, 1.2280999422073364, 1.2280999422073364, 1.2280999422073364, 1.2280000448226929, 1.2279000282287598, 1.2279000282287598, 1.2278000116348267, 1.2278000116348267, 1.2278000116348267, 1.1887999773025513, 1.1292999982833862, 1.0753999948501587, 0.8647000193595886, 0.8073999881744385, 1.0389000177383423, 0.9535999894142151, 0.6988000273704529, 1.002500057220459, 0.8799999952316284, 0.5945000052452087, 0.9352999925613403, 1.044600009918213, 0.732699990272522, 0.7297999858856201, 0.7770000100135803, 0.3089999854564667, -0.5992000102996826, 0.09260000288486481, -0.66839998960495, 0.6865000128746033, 0.8719000220298767, 1.2314000129699707, 1.2311999797821045, 1.2311999797821045, 1.2311999797821045, 1.2310999631881714, 1.2310999631881714, 1.2310999631881714, 1.2310999631881714, 1.2309999465942383, 1.2309999465942383, 1.2309999465942383, 1.2309000492095947, 1.2309000492095947, 1.2309000492095947, 1.2309000492095947, 1.2308000326156616, 1.2308000326156616, 1.2308000326156616, 1.2308000326156616, 1.2308000326156616, 1.2308000326156616, 1.2307000160217285, 1.2307000160217285, 1.2307000160217285, 1.2307000160217285, 1.2307000160217285, 1.2307000160217285, 1.2305999994277954, 1.2305999994277954, 1.2304999828338623, 1.2000000476837158, 1.068600058555603, 0.9835000038146973, 1.1052000522613525, 0.5285999774932861, 0.8475000262260437, 0.8259000182151794, 0.23070000112056732, -0.07150000333786011, 0.8440999984741211, -0.17739999294281006, -0.9821000099182129, 0.3447999954223633, -0.6759999990463257, 1.3341000080108643, 1.3341000080108643, 1.3339999914169312, 1.3339999914169312, 1.3339999914169312, 1.333899974822998, 1.333799958229065, 1.333799958229065, 1.333799958229065, 1.3336999416351318, 1.3336999416351318, 1.3336000442504883, 1.3336000442504883, 1.3336000442504883, 1.3336000442504883, 1.3335000276565552, 1.3335000276565552, 1.3335000276565552, 1.3335000276565552, 1.3335000276565552, 1.333400011062622, 1.333400011062622, 1.333400011062622, 1.333400011062622, 1.333400011062622, 1.333299994468689, 1.333299994468689, 1.333299994468689, 1.333299994468689, 1.3331999778747559, 1.1734000444412231, 1.034000039100647, 1.1511000394821167, 1.101099967956543, 0.9469000101089478, 1.0470999479293823, 1.045699954032898, 0.9854999780654907, 1.1253000497817993, 0.04349999874830246, 0.8027999997138977, 0.8331999778747559, 0.39570000767707825, 1.0664000511169434, 0.9124000072479248, -0.3345000147819519, 1.8823000192642212, 1.882099986076355, 1.8819999694824219, 1.8818999528884888, 1.8818999528884888, 1.8818000555038452, 1.8818000555038452, 1.881700038909912, 1.881600022315979, 1.881500005722046, 1.8813999891281128, 1.8813999891281128, 1.8813999891281128, 1.8812999725341797, 1.8812999725341797, 1.8812999725341797, 1.8811999559402466, 1.8809000253677368, 1.8809000253677368, 1.8808000087738037, 1.8808000087738037, 1.8808000087738037, 1.8808000087738037, 1.8808000087738037, 1.8806999921798706, 1.8806999921798706, 1.8806999921798706, 1.8805999755859375, 1.8805999755859375, 1.8805999755859375, 1.424399971961975, 1.5182000398635864, 1.6800999641418457, 1.2706999778747559, 1.2336000204086304, 1.389799952507019, 1.5533000230789185, 1.305999994277954, 1.0049999952316284, 1.1684000492095947, 1.4182000160217285, 0.2623000144958496, 0.6597999930381775, 0.5669999718666077, 1.0341999530792236, 0.7391999959945679, 0.5860000252723694, -0.7897999882698059, 0.498199999332428, -0.4250999987125397, -0.541100025177002, 0.4659999907016754], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.585299968719482, -5.3196001052856445, -5.366300106048584, -5.414599895477295, -5.461400032043457, -5.5142998695373535, -5.618000030517578, -5.638400077819824, -5.649199962615967, -5.649099826812744, -5.677499771118164, -5.6697998046875, -5.671299934387207, -5.725100040435791, -5.763400077819824, -5.754799842834473, -5.766900062561035, -5.857600212097168, -5.851500034332275, -5.867400169372559, -5.88640022277832, -5.904099941253662, -5.956999778747559, -6.022299766540527, -6.1194000244140625, -6.151299953460693, -6.206299781799316, -6.268599987030029, -6.297100067138672, -6.255099773406982, -5.583000183105469, -5.154300212860107, -5.248899936676025, -4.2835001945495605, -4.420899868011475, -5.351500034332275, -5.196100234985352, -4.457900047302246, -5.3796000480651855, -5.124100208282471, -4.547500133514404, -5.391900062561035, -5.577899932861328, -5.148499965667725, -5.196899890899658, -5.542300224304199, -5.43779993057251, -5.177800178527832, -5.441699981689453, -5.298600196838379, -5.5640997886657715, -5.637800216674805, -4.413700103759766, -5.102700233459473, -5.14169979095459, -5.166800022125244, -5.317399978637695, -5.405799865722656, -5.414899826049805, -5.4492998123168945, -5.565800189971924, -5.559500217437744, -5.657299995422363, -5.684899806976318, -5.728099822998047, -5.769100189208984, -5.821199893951416, -5.892600059509277, -5.904900074005127, -5.912700176239014, -5.907599925994873, -5.9670000076293945, -5.991799831390381, -5.992800235748291, -6.020699977874756, -6.029200077056885, -6.03980016708374, -6.042399883270264, -6.1082000732421875, -6.14709997177124, -6.174600124359131, -6.250699996948242, -4.281199932098389, -5.34060001373291, -4.974299907684326, -5.568999767303467, -4.050000190734863, -5.4980998039245605, -5.630099773406982, -5.337900161743164, -5.2281999588012695, -5.8196001052856445, -5.405799865722656, -5.612299919128418, -5.8119001388549805, -5.81879997253418, -3.882999897003174, -4.264699935913086, -4.290999889373779, -4.388500213623047, -4.533400058746338, -4.954699993133545, -5.202899932861328, -5.257599830627441, -5.3368000984191895, -5.395599842071533, -5.482500076293945, -5.56850004196167, -5.607100009918213, -5.638199806213379, -5.693699836730957, -5.752799987792969, -5.754499912261963, -5.770599842071533, -5.806600093841553, -5.848499774932861, -5.864200115203857, -5.882699966430664, -5.879199981689453, -5.9039998054504395, -5.911300182342529, -5.972899913787842, -6.0279998779296875, -6.013000011444092, -6.05049991607666, -6.125800132751465, -3.969399929046631, -3.5961999893188477, -4.583099842071533, -4.986499786376953, -4.587399959564209, -4.992599964141846, -5.17579984664917, -5.149700164794922, -5.548500061035156, -4.535099983215332, -5.353899955749512, -5.4552001953125, -5.351099967956543, -5.654300212860107, -5.624599933624268, -5.476500034332275, -4.798299789428711, -5.141900062561035, -5.249899864196777, -5.4197998046875, -5.453800201416016, -5.526100158691406, -5.502900123596191, -5.642099857330322, -5.737800121307373, -5.805500030517578, -5.876699924468994, -5.891200065612793, -5.904099941253662, -5.919600009918213, -5.933499813079834, -5.989099979400635, -6.053400039672852, -6.202700138092041, -6.195300102233887, -6.219200134277344, -6.208600044250488, -6.209199905395508, -6.235000133514404, -6.22599983215332, -6.264900207519531, -6.269400119781494, -6.32289981842041, -6.3383002281188965, -6.336999893188477, -6.322400093078613, -4.144199848175049, -5.245100021362305, -5.957900047302246, -5.388800144195557, -5.496799945831299, -5.703800201416016, -5.889800071716309, -5.636899948120117, -5.436699867248535, -5.597300052642822, -5.816199779510498, -4.879700183868408, -5.344399929046631, -5.51639986038208, -5.707399845123291, -5.606299877166748, -5.664599895477295, -5.3684000968933105, -5.723299980163574, -5.653500080108643, -5.6894001960754395, -5.853300094604492]}, \"token.table\": {\"Topic\": [3, 3, 3, 3, 3, 3, 4, 3, 1, 2, 4, 1, 2, 3, 4, 1, 4, 2, 1, 2, 3, 3, 2, 2, 2, 3, 1, 2, 3, 4, 2, 1, 1, 4, 4, 2, 3, 3, 2, 2, 4, 3, 3, 2, 3, 2, 2, 4, 3, 1, 2, 4, 1, 4, 4, 4, 1, 2, 3, 1, 3, 3, 4, 3, 4, 4, 1, 4, 1, 1, 1, 2, 1, 4, 2, 1, 2, 2, 2, 3, 4, 1, 4, 2, 3, 4, 1, 1, 3, 1, 3, 2, 3, 4, 1, 2, 3, 1, 2, 4, 4, 1, 3, 4, 1, 3, 1, 1, 2, 3, 4, 3, 4, 1, 2, 4, 1, 4, 4, 2, 2, 1, 4, 1, 3, 1, 2, 1, 2, 3, 4, 1, 1, 1, 2, 3, 3, 1, 2, 3, 4, 4, 2, 4, 1, 2, 4, 4, 2, 4, 4, 3, 4, 1, 2, 3, 2, 2, 1, 2, 4, 3, 3, 3, 2, 4, 1, 3, 4, 1, 1, 3, 1, 3, 2, 3, 2, 2, 1, 2, 1, 2, 3, 4, 1, 2, 4, 2, 1, 2, 4, 1, 2, 4, 4, 1, 4, 2, 1, 4, 2, 2, 1, 1, 1, 1, 1, 3, 1, 3, 3, 1, 2, 3, 4, 1, 2, 3, 4, 3, 3, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 2, 2, 1, 3, 4, 2, 2, 2, 1, 4, 1, 2, 1, 3, 1, 4, 1, 2, 3, 1, 1, 3, 1, 3, 3], \"Freq\": [0.99944669008255, 0.999247670173645, 0.9990764856338501, 0.9989224076271057, 0.999098002910614, 0.9994584321975708, 0.9969699382781982, 0.9999118447303772, 0.5382077693939209, 0.03311048820614815, 0.427839457988739, 0.26250916719436646, 0.07159341126680374, 0.39762914180755615, 0.26822319626808167, 0.4769939184188843, 0.522574782371521, 0.9991230368614197, 0.39849939942359924, 0.21005238592624664, 0.3912976086139679, 0.9995595812797546, 0.9995537400245667, 0.9996147751808167, 0.999389111995697, 0.9996888041496277, 0.9987621307373047, 0.9995207786560059, 0.30517736077308655, 0.6946102380752563, 0.9987974166870117, 0.99952232837677, 0.28018540143966675, 0.7187933921813965, 0.9982749223709106, 0.9995650053024292, 0.9994000196456909, 0.999824583530426, 0.9995911717414856, 0.9993422031402588, 0.9995811581611633, 0.9995207786560059, 0.9995948076248169, 0.29418227076530457, 0.7055418491363525, 0.9994663596153259, 0.9996870756149292, 0.9986532330513, 0.9998397827148438, 0.7454973459243774, 0.13297133147716522, 0.1213899552822113, 0.9991011619567871, 0.9980223774909973, 0.9990602135658264, 0.998659074306488, 0.6995800733566284, 0.3001141846179962, 0.9997250437736511, 0.3210483491420746, 0.6789764761924744, 0.38863512873649597, 0.6111079454421997, 0.9997808337211609, 0.9996950626373291, 0.9991412162780762, 0.7055234909057617, 0.2943821847438812, 0.9993633031845093, 0.9994490742683411, 0.8270355463027954, 0.17271791398525238, 0.9990417957305908, 0.999475359916687, 0.999763011932373, 0.9996944069862366, 0.9995852112770081, 0.9996203780174255, 0.8497017025947571, 0.14991997182369232, 0.9986746907234192, 0.999330461025238, 0.9991543292999268, 0.8811339735984802, 0.11833410710096359, 0.9994701147079468, 0.9994761347770691, 0.9994192123413086, 0.9992120265960693, 0.24936175346374512, 0.750337541103363, 0.4119807481765747, 0.5877688527107239, 0.9991254210472107, 0.9607568383216858, 0.030776333063840866, 0.008028608746826649, 0.6561148762702942, 0.24443495273590088, 0.09948931634426117, 0.99698406457901, 0.46173399686813354, 0.12264809757471085, 0.41556060314178467, 0.9988595247268677, 0.9995024800300598, 0.999196469783783, 0.6070060133934021, 0.2261902242898941, 0.07185204327106476, 0.09484469145536423, 0.9987136125564575, 0.9979708194732666, 0.5813080072402954, 0.1450289934873581, 0.2733697295188904, 0.9988800287246704, 0.999110221862793, 0.9982864856719971, 0.998702347278595, 0.9991688132286072, 0.9992650151252747, 0.9975511431694031, 0.9991525411605835, 0.9995690584182739, 0.797576367855072, 0.2021130919456482, 0.5302553176879883, 0.08339326828718185, 0.18855269253253937, 0.19786232709884644, 0.9985595941543579, 0.9991852641105652, 0.1499587893486023, 0.10932530462741852, 0.740676999092102, 0.9997458457946777, 0.6948037147521973, 0.10067133605480194, 0.11597654223442078, 0.08853272348642349, 0.9991963505744934, 0.6810834407806396, 0.3184796869754791, 0.9991210699081421, 0.9987860321998596, 0.9970527291297913, 0.9993219971656799, 0.9689832329750061, 0.030924996361136436, 0.9993184208869934, 0.18295128643512726, 0.8161218166351318, 0.9988231062889099, 0.2079244703054428, 0.7918682098388672, 0.9994356036186218, 0.9996423721313477, 0.9050360918045044, 0.09444569051265717, 0.9964691400527954, 0.9993685483932495, 0.9997596144676208, 0.9999139308929443, 0.36762353777885437, 0.6323928236961365, 0.1897457242012024, 0.2484954446554184, 0.5620919466018677, 0.9983953237533569, 0.9990828037261963, 0.9989462494850159, 0.34388166666030884, 0.6560204029083252, 0.1484173834323883, 0.8515300154685974, 0.9983580112457275, 0.9992802143096924, 0.8575679659843445, 0.14200128614902496, 0.5884935855865479, 0.2716943025588989, 0.0713164284825325, 0.06838925927877426, 0.6362211108207703, 0.12086073309183121, 0.2425725907087326, 0.9994428157806396, 0.21967975795269012, 0.7802930474281311, 0.9998061656951904, 0.1976054608821869, 0.6664915084838867, 0.13564029335975647, 0.9975036382675171, 0.9986706376075745, 0.9994739294052124, 0.999141275882721, 0.5100705027580261, 0.48945489525794983, 0.9993983507156372, 0.9997761845588684, 0.999354898929596, 0.9994396567344666, 0.999764621257782, 0.9994929432868958, 0.9997392296791077, 0.9992452263832092, 0.188053697347641, 0.8116640448570251, 0.9996764063835144, 0.16070379316806793, 0.4950990378856659, 0.27505502104759216, 0.06904366612434387, 0.13692450523376465, 0.6786170601844788, 0.18436764180660248, 0.9985907673835754, 0.9994533061981201, 0.9995137453079224, 0.6087518930435181, 0.1293288916349411, 0.258383184671402, 0.003295003669336438, 0.07118995487689972, 0.2592589259147644, 0.040376391261816025, 0.627959132194519, 0.749411404132843, 0.25044697523117065, 0.9994471073150635, 0.9996147155761719, 0.26430031657218933, 0.19374050199985504, 0.5423538088798523, 0.9997508525848389, 0.9998486042022705, 0.9989104270935059, 0.9990874528884888, 0.9992637634277344, 0.7593057751655579, 0.24029119312763214, 0.23459164798259735, 0.7648069262504578, 0.9995861649513245, 0.9975903630256653, 0.8315978050231934, 0.014407445676624775, 0.15387152135372162, 0.9994895458221436, 0.1671367883682251, 0.8326019644737244, 0.3940143883228302, 0.6060807704925537, 0.9989908337593079], \"Term\": [\"1\", \"10\", \"2\", \"2014\", \"2015\", \"2016\", \"able\", \"album\", \"already\", \"already\", \"already\", \"also\", \"also\", \"also\", \"also\", \"artists\", \"artists\", \"b\", \"back\", \"back\", \"back\", \"based\", \"bass\", \"beat\", \"beats\", \"called\", \"cant\", \"catchy\", \"check\", \"check\", \"chorus\", \"could\", \"cover\", \"cover\", \"creative\", \"dance\", \"dates\", \"debut\", \"deep\", \"disco\", \"download\", \"dropped\", \"due\", \"duo\", \"duo\", \"electro\", \"electronic\", \"enjoy\", \"ep\", \"even\", \"even\", \"even\", \"every\", \"everyone\", \"facebook\", \"favourite\", \"feel\", \"feel\", \"festival\", \"first\", \"first\", \"follow\", \"follow\", \"forthcoming\", \"free\", \"friends\", \"get\", \"get\", \"getting\", \"going\", \"good\", \"good\", \"got\", \"grab\", \"guitar\", \"heard\", \"hip\", \"hop\", \"house\", \"house\", \"hype\", \"im\", \"immediately\", \"indie\", \"indie\", \"itunes\", \"ive\", \"know\", \"label\", \"last\", \"last\", \"latest\", \"latest\", \"let\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"list\", \"listen\", \"listen\", \"listen\", \"listening\", \"london\", \"lot\", \"love\", \"love\", \"love\", \"love\", \"lp\", \"machine\", \"make\", \"make\", \"make\", \"many\", \"matter\", \"media\", \"melodies\", \"melody\", \"might\", \"million\", \"moment\", \"month\", \"much\", \"much\", \"music\", \"music\", \"music\", \"music\", \"need\", \"never\", \"new\", \"new\", \"new\", \"october\", \"one\", \"one\", \"one\", \"one\", \"order\", \"original\", \"original\", \"people\", \"piano\", \"pick\", \"plays\", \"pop\", \"pop\", \"posted\", \"pre\", \"pre\", \"pretty\", \"producer\", \"producer\", \"production\", \"r\", \"really\", \"really\", \"recordings\", \"records\", \"release\", \"released\", \"remix\", \"remix\", \"remixes\", \"remixes\", \"remixes\", \"said\", \"say\", \"september\", \"since\", \"since\", \"single\", \"single\", \"slow\", \"smooth\", \"something\", \"something\", \"song\", \"song\", \"song\", \"song\", \"songs\", \"songs\", \"songs\", \"soul\", \"sound\", \"sound\", \"soundcloud\", \"sounds\", \"sounds\", \"sounds\", \"spotify\", \"start\", \"stream\", \"style\", \"sure\", \"sure\", \"synth\", \"synths\", \"thing\", \"things\", \"think\", \"though\", \"time\", \"titled\", \"today\", \"today\", \"tour\", \"track\", \"track\", \"track\", \"track\", \"tune\", \"tune\", \"tune\", \"twitter\", \"uk\", \"upcoming\", \"us\", \"us\", \"us\", \"us\", \"version\", \"version\", \"version\", \"version\", \"via\", \"via\", \"vibe\", \"vibes\", \"video\", \"video\", \"video\", \"vocal\", \"vocals\", \"voice\", \"want\", \"watch\", \"way\", \"way\", \"week\", \"week\", \"weve\", \"white\", \"world\", \"world\", \"world\", \"would\", \"year\", \"year\", \"years\", \"years\", \"york\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el252351404985145292807646282134\", ldavis_el252351404985145292807646282134_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el252351404985145292807646282134\", ldavis_el252351404985145292807646282134_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el252351404985145292807646282134\", ldavis_el252351404985145292807646282134_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.076924 -0.082862       1        1  29.261843\n",
       "3     -0.107507 -0.229203       2        1  29.184538\n",
       "2     -0.151252  0.278826       3        1  26.336664\n",
       "1      0.335682  0.033240       4        1  15.216954, topic_info=     Category          Freq         Term         Total  loglift  logprob\n",
       "term                                                                    \n",
       "0     Default   7074.000000        album   7074.000000  30.0000  30.0000\n",
       "21    Default  12723.000000          new  12723.000000  29.0000  29.0000\n",
       "13    Default   7620.000000       single   7620.000000  28.0000  28.0000\n",
       "174   Default   4830.000000      release   4830.000000  27.0000  27.0000\n",
       "114   Default   4704.000000           ep   4704.000000  26.0000  26.0000\n",
       "185   Default   4977.000000        remix   4977.000000  25.0000  25.0000\n",
       "565   Default   5432.000000          pop   5432.000000  24.0000  24.0000\n",
       "4     Default   4267.000000        debut   4267.000000  23.0000  23.0000\n",
       "256   Default   4611.000000       vocals   4611.000000  22.0000  22.0000\n",
       "385   Default   3692.000000     released   3692.000000  21.0000  21.0000\n",
       "216   Default   3895.000000         time   3895.000000  20.0000  20.0000\n",
       "188   Default   4218.000000         year   4218.000000  19.0000  19.0000\n",
       "209   Default   5151.000000        first   5151.000000  18.0000  18.0000\n",
       "504   Default   2423.000000        based   2423.000000  17.0000  17.0000\n",
       "56    Default   1637.000000   soundcloud   1637.000000  16.0000  16.0000\n",
       "833   Default   2315.000000   electronic   2315.000000  15.0000  15.0000\n",
       "75    Default   6995.000000         like   6995.000000  14.0000  14.0000\n",
       "37    Default   2227.000000   production   2227.000000  13.0000  13.0000\n",
       "577   Default   3220.000000          get   3220.000000  12.0000  12.0000\n",
       "572   Default   2171.000000        dance   2171.000000  11.0000  11.0000\n",
       "14    Default   1890.000000         tour   1890.000000  10.0000  10.0000\n",
       "429   Default   2962.000000     producer   2962.000000   9.0000   9.0000\n",
       "184   Default   3107.000000         last   3107.000000   8.0000   8.0000\n",
       "862   Default   1790.000000      records   1790.000000   7.0000   7.0000\n",
       "103   Default   3373.000000        sound   3373.000000   6.0000   6.0000\n",
       "95    Default   1868.000000        vocal   1868.000000   5.0000   5.0000\n",
       "847   Default   1869.000000           im   1869.000000   4.0000   4.0000\n",
       "121   Default   2591.000000          via   2591.000000   3.0000   3.0000\n",
       "98    Default   7579.000000          one   7579.000000   2.0000   2.0000\n",
       "38    Default   2435.000000       really   2435.000000   1.0000   1.0000\n",
       "...       ...           ...          ...           ...      ...      ...\n",
       "1203   Topic4    389.034241      machine    389.790955   1.8808  -6.2350\n",
       "2768   Topic4    392.542908  immediately    393.332642   1.8808  -6.2260\n",
       "1638   Topic4    377.563690       matter    378.336639   1.8807  -6.2649\n",
       "1071   Topic4    375.866974     creative    376.649750   1.8807  -6.2694\n",
       "3393   Topic4    356.300568         pick    357.052338   1.8807  -6.3229\n",
       "638    Topic4    350.857391        media    351.602478   1.8806  -6.3383\n",
       "2569   Topic4    351.301697         list    352.061798   1.8806  -6.3370\n",
       "771    Topic4    356.482880   recordings    357.261444   1.8806  -6.3224\n",
       "185    Topic4   3147.632080        remix   4977.918457   1.4244  -4.1442\n",
       "611    Topic4   1046.843140        check   1507.320190   1.5182  -5.2451\n",
       "521    Topic4    513.255188          pre    628.582642   1.6801  -5.9579\n",
       "217    Topic4    906.760498        video   1672.340088   1.2707  -5.3888\n",
       "18     Topic4    813.864929      artists   1557.671875   1.2336  -5.4968\n",
       "570    Topic4    661.698242       follow   1083.278320   1.3898  -5.7038\n",
       "918    Topic4    549.384399        cover    763.779968   1.5533  -5.8898\n",
       "500    Topic4    707.538025      remixes   1259.580444   1.3060  -5.6369\n",
       "408    Topic4    864.350220       listen   2079.119141   1.0050  -5.4367\n",
       "344    Topic4    736.046570         sure   1503.713623   1.1684  -5.5973\n",
       "961    Topic4    591.399048      version    941.144043   1.4182  -5.8162\n",
       "20     Topic4   1508.533325        music   7626.515137   0.2623  -4.8797\n",
       "577    Topic4    947.896545          get   3220.303711   0.6598  -5.3444\n",
       "145    Topic4    798.133728         also   2975.134033   0.5670  -5.5164\n",
       "853    Topic4    659.310669      already   1540.297363   1.0342  -5.7074\n",
       "749    Topic4    729.480591     original   2289.000000   0.7392  -5.6063\n",
       "1023   Topic4    688.142883         make   2516.738037   0.5860  -5.6646\n",
       "41     Topic4    925.452454        track  13397.319336  -0.7898  -5.3684\n",
       "121    Topic4    648.958435          via   2591.366943   0.4982  -5.7233\n",
       "75     Topic4    695.871399         like   6995.726074  -0.4251  -5.6535\n",
       "98     Topic4    671.347473          one   7579.118652  -0.5411  -5.6894\n",
       "214    Topic4    569.811462        songs   2349.812012   0.4660  -5.8533\n",
       "\n",
       "[224 rows x 6 columns], token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "2108      3  0.999447        1\n",
       "3127      3  0.999248       10\n",
       "389       3  0.999076        2\n",
       "926       3  0.998922     2014\n",
       "2110      3  0.999098     2015\n",
       "994       3  0.999458     2016\n",
       "2140      4  0.996970     able\n",
       "0         3  0.999912    album\n",
       "853       1  0.538208  already\n",
       "853       2  0.033110  already\n",
       "853       4  0.427839  already\n",
       "145       1  0.262509     also\n",
       "145       2  0.071593     also\n",
       "145       3  0.397629     also\n",
       "145       4  0.268223     also\n",
       "18        1  0.476994  artists\n",
       "18        4  0.522575  artists\n",
       "124       2  0.999123        b\n",
       "219       1  0.398499     back\n",
       "219       2  0.210052     back\n",
       "219       3  0.391298     back\n",
       "504       3  0.999560    based\n",
       "59        2  0.999554     bass\n",
       "1083      2  0.999615     beat\n",
       "871       2  0.999389    beats\n",
       "418       3  0.999689   called\n",
       "912       1  0.998762     cant\n",
       "526       2  0.999521   catchy\n",
       "611       3  0.305177    check\n",
       "611       4  0.694610    check\n",
       "...     ...       ...      ...\n",
       "961       2  0.259259  version\n",
       "961       3  0.040376  version\n",
       "961       4  0.627959  version\n",
       "121       3  0.749411      via\n",
       "121       4  0.250447      via\n",
       "402       2  0.999447     vibe\n",
       "324       2  0.999615    vibes\n",
       "217       1  0.264300    video\n",
       "217       3  0.193741    video\n",
       "217       4  0.542354    video\n",
       "95        2  0.999751    vocal\n",
       "256       2  0.999849   vocals\n",
       "823       2  0.998910    voice\n",
       "956       1  0.999087     want\n",
       "1338      4  0.999264    watch\n",
       "534       1  0.759306      way\n",
       "534       2  0.240291      way\n",
       "671       1  0.234592     week\n",
       "671       3  0.764807     week\n",
       "837       1  0.999586     weve\n",
       "990       4  0.997590    white\n",
       "325       1  0.831598    world\n",
       "325       2  0.014407    world\n",
       "325       3  0.153872    world\n",
       "890       1  0.999490    would\n",
       "188       1  0.167137     year\n",
       "188       3  0.832602     year\n",
       "394       1  0.394014    years\n",
       "394       3  0.606081    years\n",
       "364       3  0.998991     york\n",
       "\n",
       "[256 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 4, 3, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "pyLDAvis.gensim.prepare(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 27466, 1: 37003, 4: 30289, 2: 9709})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([max(lda[corpus[i]], key=itemgetter(1))[0] + 1 for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sorted([f'{max(lda[corpus[i]], key=itemgetter(1))} {ds.normalize_string(sents[i])}\\n' for i in range(len(texts))], reverse=True)\n",
    "with open('pairs_sentence_topics.txt', 'w') as f:\n",
    "    f.writelines(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^using this file to select desirable/preferred topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(i, pairs, input_side, output_side, random_state, device):\n",
    "    pair = pairs[i]\n",
    "\n",
    "    inp = model.word_tensor(input_side, pair[0]).to(device)\n",
    "    target = model.word_tensor(output_side, pair[1]).to(device)\n",
    "    condition = torch.tensor(pair[2], dtype=torch.float).unsqueeze(0).to(device) if len(pair) == 3 else None\n",
    "\n",
    "    return inp, target, condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 76.54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.6759042739868164,\n",
       "  'that is to say she clearly possesses a fine set of lungs and her songs mash up a number of genres in bishops case acoustic folk dark electronic pop and soul'),\n",
       " (0.0,\n",
       "  '2010s swim lp did this in incredible navigating a massive sonic palette in 40 minutes flat exposing every part of the human condition to a rawness that we otherwise don t come into contact with frequently'),\n",
       " (-0.5707800984382629,\n",
       "  'the full moon from my birthday is waning i conserved as much as i could'),\n",
       " (-0.5841445326805115,\n",
       "  'with two full albums numerous singles and eps under his belt the young artist and label boss is further challenging himself and pushing forward with his first live recorded ep called reverie'),\n",
       " (0.0,\n",
       "  'with a mashup name of three of my favorite things i assumed they sounded like a meme'),\n",
       " (0.0, 'one could even say things are waft as well as emanate'),\n",
       " (0.0,\n",
       "  'theres no release date available for this one but feel free to stream it all you want right here below'),\n",
       " (0.0,\n",
       "  'its a much better look for him even without a proper album for it to sit on'),\n",
       " (0.0, 'he says as soon as he gets back from the hunt we can switch'),\n",
       " (0.0,\n",
       "  'luckily i happened to listen to argonaut wasps song_title a track which did an even better job of waking me up than the caffeine'),\n",
       " (0.0,\n",
       "  'to not include this one in your 2015s hottest tropical house songs playlist would be a big mistake'),\n",
       " (0.0,\n",
       "  'besides taking alinas label to task for pulling his credit elsewhere he responded to youtube comments on his tracks passionately and actually changed his profile name to dying narwhale adopting the moniker one critic used to describe his singing'),\n",
       " (0.9458534121513367,\n",
       "  'hertzs mellow and harmonious beats goes hand in hand absolutely perfectly with bassettes smooth and sultry vocals'),\n",
       " (0.0,\n",
       "  'this is not a band to mess with and youll know its them the moment that vocal kicks in'),\n",
       " (0.0,\n",
       "  'this is the kind of song that trigger memories from the best of times hanging with friends late night adventures weekend pool parties whatever good time it is years from now youll hear this song and those memories will come rushing back and a quiet smile will appear'),\n",
       " (0.6016775965690613,\n",
       "  'new single voodoo sees a sassy carefree side to the artist'),\n",
       " (0.0,\n",
       "  'the first ten seconds of this song trick you into thinking its going to be a trap banger'),\n",
       " (-0.5096626281738281,\n",
       "  'shes had a few releases since then and ive kept half an eye on them though it wasnt until this week that she managed to sway me over with a new one'),\n",
       " (0.0, 'i wanted to give you an uncompromising experience at every turn'),\n",
       " (0.0,\n",
       "  'thats where listen2liri has come in remixing the track to give it wider appeal and a much more fun loving feel'),\n",
       " (-0.3528803884983063,\n",
       "  'its been over a decade since artist dropped bright like neon love and todays featured track proves these dudes still have juice in the tank'),\n",
       " (0.4316568076610565,\n",
       "  'that could have happened with artist s euphoric summer 2013 record entitled neon because frankly it took three years to produce the perfect melody wielding track and bring it to perfection for release'),\n",
       " (0.46104246377944946,\n",
       "  'moving away from their more dubstep inclined outings artist have revealed a new piece of chillwave showing off their subtly and a loungey capability that will help widen their ever growing appeal to a wider audience'),\n",
       " (0.0,\n",
       "  'grated through littlest holes in the grater 1 2 t turmeric 1 2 t cinnamon pinch of black pepper or cayenne honey to taste tsp coconut oil optional large lemon wedge optional put the ginger and all the spices in a tea strainer and steep for 10 minutes then add honey lemon and coconut oil i read somewhere that fat will help the good things in the turmeric absorb which who knows if that s true but also its nice because it makes it kind of creamy'),\n",
       " (-0.6677809357643127,\n",
       "  'after what seems like an eternity actually about six months melbourne maestro fractures returns with brand new music'),\n",
       " (-0.9365429282188416, 'w sam smith 11 16 14 live music hall cologne germany'),\n",
       " (0.0,\n",
       "  'i got a bit bored of writing reports on inequality so figured it was the perfect time to sit down and have a look at my overflowing inbox'),\n",
       " (-0.37764114141464233,\n",
       "  'for la singer artist she reached that milestone with the release of her premiere song filthy rich which not only garnered 3 million streams on spotify but has also kept its momentum going with the release of a sweater beats remix'),\n",
       " (0.0,\n",
       "  'of the track the band say song_title goes out to all those fake ass people'),\n",
       " (0.5363920331001282,\n",
       "  'it has a little diplo vibe in a great way but has enough unpredictable elements that after a few listens you forget its a remix'),\n",
       " (0.0, 'well let you decide where you stand on it but the song is cracking'),\n",
       " (0.524086594581604, 'kirk fits a lot of voices into this tracks tight 4 43'),\n",
       " (0.0,\n",
       "  'todd terje has remixed ola kvernbergs the mechanical fair due to come out in november on his olsen label'),\n",
       " (-0.824548602104187,\n",
       "  'artist s debut album titled id is out june 25 via dovecote records'),\n",
       " (0.4011840522289276,\n",
       "  'let s eat grandma are the british duo that involves multi instrumentalists rosa walton and jenny hollingworth'),\n",
       " (0.0,\n",
       "  'i really like artist and after posting their single wings a whole nine months ago wow time flies ive kept track of everything theyve done since'),\n",
       " (0.0,\n",
       "  'in the value delivers a funked out fuzziness that could be kin to chet faker and also serve as bubbly baby making music to san diegos ron burgundy'),\n",
       " (0.0,\n",
       "  'its about letting go of a past relationship and finding myself in the process'),\n",
       " (0.6502144932746887,\n",
       "  'this remix sees a meeting of the two with hudson giving parallel jalebi club ready reworking with a typically heavy low end'),\n",
       " (0.0,\n",
       "  'artist s self titled debut album often felt like an intimate conversation between friends'),\n",
       " (0.0, 'check out those dates here and go catch him if you can'),\n",
       " (0.0,\n",
       "  'even through cliches like you never know whats special till its gone fyfe manages to sound sincere and pained'),\n",
       " (-0.8495330810546875,\n",
       "  'song_title is the first single to be released off the album'),\n",
       " (0.0,\n",
       "  'it kind of fits _starfire _s music which you could say is both cosmic and carnal'),\n",
       " (0.0,\n",
       "  'the same is true of the flip and i dont see anything noble about denying yourself things you enjoy'),\n",
       " (0.0,\n",
       "  'on song_title and hopefully what follows it we get a bit more personality'),\n",
       " (0.0,\n",
       "  'whether were desensitised to the formula by now ravenous for anything the chic punk p triumvirate might turn out or not kudos to pharrell for tapping the psyche so goddamn well'),\n",
       " (0.0,\n",
       "  'song_title is his best yet displaying his sophisticated writing skills whereby i mean lyrically using a lot of words of a high vocabulary caliber to help paint his picture and an awareness of both old and new popular music trends'),\n",
       " (0.0, 'theyre easily one of the best bands ive seen live this decade'),\n",
       " (-0.474514901638031,\n",
       "  'after living and performing in montreal and halifax both with band project the wayo and as a solo artist artist chose to return to her native toronto in search of what she calls her musical home'),\n",
       " (0.0,\n",
       "  'given an unlimited budget hed like to bring a custom speaker system on tour with them for 360deg sound'),\n",
       " (0.0,\n",
       "  'the album features eight tracks as listed below and you can hear previews here'),\n",
       " (-0.6713810563087463,\n",
       "  'i wish i could which appeared on her latest album that dropped back in october called lune rouge'),\n",
       " (-0.32178235054016113,\n",
       "  'keep your wednesday nice and smooth with tom mischs remix of midnight mischief by'),\n",
       " (-0.7310837507247925,\n",
       "  'over the weekend artist dropped three new tunes song_title heat of the moment and the previously leaked how about now'),\n",
       " (0.7324954867362976,\n",
       "  'its easy for artists to fall into formation when it comes to edm electropop and most sorts of club pleasing dance oriented genre'),\n",
       " (0.39752405881881714,\n",
       "  'but it does swell like a ripe peach as the wide open spaces are filled in by sounds that really warm your cockles and make this into something youre not getting down your local pub even if they serve pie and mash'),\n",
       " (0.6727477312088013,\n",
       "  'artist is a fresh face in the dance music scene and she is bringing a great new flavor of edm'),\n",
       " (0.618151068687439,\n",
       "  'reworking next year off northern irish indie rock band artist s second studio album beacon rac lets the vocals shine by removing much of the original instrumentation and sprinkling it with colorful and vivid pop layers of pure bliss'),\n",
       " (0.3642151951789856,\n",
       "  'written and recorded in ibiza with duran duran s anthony taylor the new song is 70s disco laced with philadelphia soul a far cry from the ting tings founding brassy dance pop and as some fans pointed out a dead ringer for daft punk s own throwback get lucky'),\n",
       " (0.6006275415420532,\n",
       "  'this is a mature sound which i have no doubt will resonate throughout their album'),\n",
       " (0.0,\n",
       "  'the whispy and effortless delivery is irresistible shes definitely one to watch as we prepare to enter the new year'),\n",
       " (0.0,\n",
       "  'the outcome of this is undoubtedly the most fun track weve heard in a while'),\n",
       " (0.6481269001960754,\n",
       "  'while the productions great and the momentum pulls the listener along incessantly all of that seems secondary when faced with the remarkable vocal melody'),\n",
       " (0.0,\n",
       "  'i guess that time still exists because ill admit i was quite giddy to turn artist'),\n",
       " (0.6885240077972412,\n",
       "  'having already achieved massive levels of success in his native belgium his latest track follows a more beat driven approach to his alternative r b with his video reflecting the wonder and mystery of the night the song weaves in and out with its oscillating synth cut and his sleepy r b moan on hip hop drums'),\n",
       " (0.0, 'what is the message you hope people take from watching the video'),\n",
       " (-0.6289025545120239,\n",
       "  'in february b3 covered top talent singer songwriter artist for his refreshingly soulful rudimental esque joker produced debut ep track called alchemy'),\n",
       " (0.0,\n",
       "  'i think the instrumental version of this song was released a couple of weeks ago but it wasnt until today that i noticed that it had received some pretty handy vocals from youngblood hawke to take it to the next level'),\n",
       " (0.0,\n",
       "  'paoro laments over how peoples lingering memories affect them i wish you could see your history is jading you song_title'),\n",
       " (0.0,\n",
       "  'the london based band brings whats left unsaid to the foreground and delivers on all fronts'),\n",
       " (0.0,\n",
       "  'after introducing herself and her band a couple songs into their set cardin added were going to play some love songs for you'),\n",
       " (-0.4200999140739441,\n",
       "  'following on from the producers breakthrough illuminate and holding on singles this latest track sees him reverting back to more reclusive gloomy electronics'),\n",
       " (0.0,\n",
       "  'were over the moon to have his interpretation of arty boy out in the world'),\n",
       " (0.3948274850845337,\n",
       "  'tinashe boss the best word to describe this track and 90 percent of the songs hemsworth touches is sexy'),\n",
       " (-0.35396477580070496,\n",
       "  'the track is a reworking of artist s talk talk which was initially released on the london based singers vacant space ep'),\n",
       " (0.9521105289459229,\n",
       "  'this new piece blends his pop dance sound with elements of oldschool rock turning hangover love into a catchy tune with an edge'),\n",
       " (0.0,\n",
       "  'this week features rf all stars the chainsmokers and viceroy plus a surprising remix of portugal'),\n",
       " (0.6576441526412964,\n",
       "  'the vocal is heard in the distance and gets closer as the finder snaps come out of the dark and into the sunlight'),\n",
       " (0.0,\n",
       "  'there are so many great lyrics and melodies from that era and dressed up a little differently they can still be a really great pop song'),\n",
       " (-0.7112465500831604,\n",
       "  'no future is available everywhere now through ultra records and be sure to click the tour poster below for all of the ticketing info'),\n",
       " (0.653147280216217,\n",
       "  'despite being fundamentally upbeat rhythm wise artist s vocalist jill lamoureuxs soaring voice has a slightly icy delivery as she laments poignant lyricism dented by disappointment'),\n",
       " (-0.42454227805137634,\n",
       "  'the british songstress catapulted herself into the spotlight with last years magnetic and carried that momentum further with it s poignant follow up'),\n",
       " (0.0,\n",
       "  'every once in a while id step outside the building in the middle of the day to make a phone call or catch a breath of fresh air that high up you realize theres no actual fresh air entering your lungs anymore just recycled and take note of the surroundings'),\n",
       " (0.0,\n",
       "  'lucky for us though the freelife boys are back at it and this time theyve brought the energy you need to get through the rest of your work week'),\n",
       " (0.0, 'i hate waking up without him by my side she states'),\n",
       " (0.0,\n",
       "  'not only lyrically but melodically the song captures the essence of a windows down late night summer drive through a big city'),\n",
       " (0.4689315855503082,\n",
       "  'in sees the 19 year old singer songwriter producer brings to the table more r b tinged beats with this latest offering the first of four new tracks is another ambient filled tightly produced track from bain that leaves you pining to hear more from the rising star'),\n",
       " (0.0,\n",
       "  'luckily i went to a college hook em horns in a city that prides itself on being the music capital of the world'),\n",
       " (-0.44551339745521545,\n",
       "  'a swaying new one and another gem by afie jurvanen off his forthcoming album which i believe is out in august'),\n",
       " (0.7440521121025085,\n",
       "  'the band consists of a j jackson lead vocals guitar aaron sharp lead guitar dak bass and greg erwin drums'),\n",
       " (0.0,\n",
       "  'he says girls your age never mean what they say _boldly breeze the vocals in a creative and charismatic hook'),\n",
       " (0.0,\n",
       "  'and we doth gaze upon their glory and lo we said aye these are pretty special alright'),\n",
       " (0.41485026478767395,\n",
       "  'where chance is meandering arbitrary and occasionally explosive artist is a well oiled machine disciplined in his approach to rap but enough of a veteran to recognise the need for clear air on the chorus'),\n",
       " (0.0,\n",
       "  'its about having the courage to risk something thats alright for the chances of getting something you really dream about'),\n",
       " (0.0, 'the studio was so loud that it shook the whole room when i played it'),\n",
       " (0.0,\n",
       "  'oh and that in between they would win 8 brit awards 7 grammys and sell over 70 million of their albums in a time when the sale of albums was supposed to be a thing of the past'),\n",
       " (-0.6213510036468506,\n",
       "  'now he announces the album on which drugs appears with a hazy voyeuristic new video for the lead single'),\n",
       " (-0.36648744344711304,\n",
       "  'yr heart is part of saddle creek records document series which aims to shine a spotlight on local music communities and its a lovely and wilting five minute swirl'),\n",
       " (0.6793286204338074,\n",
       "  'a cool mix of rhodes driven introspection and soft touch percussion and ms wilson s stunning alto song_title')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from pytorchtextvae import model\n",
    "\n",
    "good_topics = [3]\n",
    "bad_topics = [2]\n",
    "\n",
    "labels = np.zeros(len(texts), dtype=float)\n",
    "embeds = []\n",
    "start = time.time()\n",
    "\n",
    "# debug vars\n",
    "ts = []\n",
    "iss = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    pair_i = random_state.choice(len(pairs))\n",
    "    iss.append(pair_i)\n",
    "    ts.append(pairs[pair_i][0])\n",
    "    input, target, condition = get_example(pair_i, pairs, input_side, output_side, random_state, DEVICE)\n",
    "    with torch.no_grad():\n",
    "        _, _, z, _ = vae(input, target, condition, DEVICE, temp)\n",
    "        squashed_condition = vae.decoder.c2h(condition)\n",
    "        decode_embed = torch.cat([z, squashed_condition], 1)\n",
    "        embeds.append(decode_embed)\n",
    "    \n",
    "    topic_tuple = max(lda[corpus[pair_i]],key=itemgetter(1))\n",
    "    topic_i = topic_tuple[0]\n",
    "    topic_p = topic_tuple[1]\n",
    "    if topic_i in good_topics:\n",
    "        labels[pair_i] = 1 * topic_p\n",
    "    elif topic_i in bad_topics:\n",
    "        labels[pair_i] = -1 * topic_p\n",
    "    else:\n",
    "        labels[pair_i] = 0\n",
    "\n",
    "print(f'runtime: {time.time() - start:.2f}s')\n",
    "list(zip([labels[j] for j in iss], ts[:n_samples]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "embed_size = embeds[0].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.dataset import *\n",
    "\n",
    "class LatentDataset(Dataset):\n",
    "    def __init__(self, embeds, labels): self.embeds,self.labels = embeds,labels\n",
    "    def __getitem__(self, idx): return A(self.embeds[idx], self.labels[idx])\n",
    "    def __len__(self): return len(self.embeds)\n",
    "    \n",
    "ds = LatentDataset(embeds, labels.astype(float))\n",
    "dl = DataLoader(ds, batch_size)\n",
    "md = ModelData('.', dl, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[-0.47296,  1.27055, -1.47337,  0.21384, -0.1532 , -2.14662, -0.59251,  1.80712, -0.35052, -0.39137,\n",
       "          -1.68437, -1.84729, -0.81128, -1.32009, -0.2597 , -1.88392, -0.4858 ,  0.08199, -0.0019 ,  0.77219,\n",
       "           1.46745,  1.29288, -1.30003,  0.10202,  0.21321,  0.70391, -0.80032, -0.66879, -1.12718, -1.633  ,\n",
       "          -0.09753,  0.33278,  0.87662, -1.30657,  1.60888,  0.94579, -0.69576,  2.02654, -2.54365,  0.83272,\n",
       "          -1.02341,  0.24143,  1.72799,  2.58184, -2.52782,  0.58085, -0.90865, -0.55949,  0.2918 ,  0.29904,\n",
       "          -2.32149, -2.06861,  1.21404,  3.55821,  0.15169,  2.69022,  1.82198, -0.54856,  0.53585,  1.13747,\n",
       "          -1.32447, -0.05488, -1.05523, -0.88538, -0.24148, -2.24305,  2.00144, -0.31771, -0.1435 ,  3.3829 ,\n",
       "           0.41342,  2.23526, -1.3962 ,  0.94955,  2.78893, -1.03265, -0.35991,  1.95806, -1.12605, -2.51646,\n",
       "           0.897  ,  0.23721,  1.34856, -0.13702, -2.33176,  2.82437,  2.31462, -0.03991,  0.51431,  1.47913,\n",
       "           1.17253, -0.55432, -2.22365, -0.9749 , -0.39445, -1.19292,  1.69098, -0.38283,  2.50049, -2.0729 ,\n",
       "          -1.19182,  1.00093, -0.44963, -1.41124, -0.48846,  0.01333, -0.22278, -0.805  , -0.00444, -0.38554,\n",
       "          -0.06708, -0.96256, -1.37506, -0.22801, -0.06524, -1.45648,  0.41255, -0.28258,  0.33238, -0.19125,\n",
       "           1.9586 ,  0.45562,  1.81022,  1.23527, -0.96719,  2.13766, -2.26597, -0.47944, -0.56032,  0.18568,\n",
       "           0.74063, -0.20534, -0.56559, -0.32065,  0.18447,  0.42646,  0.4672 , -0.02403, -0.50998,  0.08574,\n",
       "           0.53045,  0.72996, -0.42296, -0.29449]], dtype=float32), array(0.)],\n",
       " (1, 144))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.trn_ds[0], md.trn_ds[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 1024\n",
    "lr = 3e-4\n",
    "fixed_genres = torch.FloatTensor(dataset.encode_genres(['edm', 'tropical house'])).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "class LCGAN_D(nn.Module):\n",
    "    '''Discriminator'''\n",
    "    def __init__(self, n_embed, n_hidden=n_hidden, n_output=1):\n",
    "        super(LCGAN_D, self).__init__()\n",
    "        \n",
    "        self.i2h = nn.Linear(n_embed, n_hidden)\n",
    "        self.h2h = nn.Linear(n_hidden, n_hidden)\n",
    "        self.h2o = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, emb):\n",
    "        x = emb\n",
    "        x = self.relu(self.i2h(x))\n",
    "        x = self.relu(self.h2h(x))\n",
    "        x = self.relu(self.h2h(x))\n",
    "        v = self.sigmoid(self.h2o(x))\n",
    "        \n",
    "        return v\n",
    "\n",
    "class LCGAN_G(nn.Module):\n",
    "    '''Generator'''\n",
    "    def __init__(self, n_embed, n_hidden=n_hidden):\n",
    "        super(LCGAN_G, self).__init__()\n",
    "        self.n_embed = n_embed\n",
    "        \n",
    "        self.i2h = nn.Linear(n_embed, n_hidden)\n",
    "        self.h2h = nn.Linear(n_hidden, n_hidden)\n",
    "        # hidden-to-gating mechanism\n",
    "        self.h2g = nn.Linear(n_hidden, 2*n_embed)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, emb):\n",
    "        x = emb\n",
    "        x = self.relu(self.i2h(x))\n",
    "        x = self.relu(self.h2h(x))\n",
    "        x = self.relu(self.h2h(x))\n",
    "        x = self.h2g(x)\n",
    "        \n",
    "        # gating mechanism: allow network to remember/forget\n",
    "        # what it wants to about the original emb(edding) and x\n",
    "        emb_mid = x[:, self.n_embed:]\n",
    "        gates = self.sigmoid(x[:, :self.n_embed])\n",
    "        demb = gates * emb_mid # TODO: why naming?\n",
    "        emb_prime = (1 - gates)*emb + demb\n",
    "        \n",
    "        return emb_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LCGAN(nn.Module):\n",
    "    def __init__(self, D, G, batch_size=batch_size):\n",
    "        super(LCGAN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.D = D\n",
    "        self.G = G\n",
    "\n",
    "    def train(self):\n",
    "        self.D.train()\n",
    "        self.G.train()\n",
    "        \n",
    "    def eval(self):\n",
    "        self.D.eval()\n",
    "        self.G.eval()\n",
    "        \n",
    "    def forward(self, emb=None):       \n",
    "        if emb is not None:\n",
    "            # train discriminator\n",
    "            #embed = to_embed(z, fixed_genres)\n",
    "            v = self.D(emb)\n",
    "            return v\n",
    "        else: # train GAN\n",
    "            # gaussian random noise\n",
    "            emb_prior = torch.randn(self.batch_size, self.G.n_embed).to(DEVICE)\n",
    "            \n",
    "            emb_prime = self.G(emb_prior)\n",
    "            v_prime = self.D(emb_prime)\n",
    "            \n",
    "            return v_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = LCGAN(LCGAN_D(embed_size).to(DEVICE), LCGAN_G(embed_size).to(DEVICE)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "\n",
    "fastai.core.set_trainable(gan.D, True)\n",
    "fastai.core.set_trainable(gan.G, True)\n",
    "\n",
    "opt_d = optim.Adam(gan.D.parameters(), lr=lr)\n",
    "opt_g = optim.Adam(gan.G.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK UNK UNK to UNK taken of the below when it builds into a warm and light on these days after far too\n",
      "that youve come out of the UNK and has a ability to familiar makes this new atmosphere\n",
      "UNK are a bit of a place where UNK are back in addition to this one that weve featured on this live list\n",
      "i listened UNK UNK is the slew of emotion and it featured on the los track if its most fun\n",
      "i felt UNK a fan of the as he come back with your UNK and it gets stuck on your face\n",
      "artist artist UNK UNK UNK of the UNK really impressed us in this world as he also featured on his radio radio\n",
      "artist is back in up and one of the of artist has just dropped a brand new track on earlier today\n",
      "this is one of the artist manage to come over five years years and which will featured on fantastic club space\n",
      "i listened to UNK inspired by the UNK and come back in a in UNK for his first song\n",
      "over the right back to UNK and UNK taking a bit of UNK and described as an radio show\n"
     ]
    }
   ],
   "source": [
    "# test what the GAN is doing before any training\n",
    "for i in range(10):\n",
    "    print(generate(fixed_genres, gan)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm\n",
    "\n",
    "# adapted from: https://github.com/fastai/fastai/blob/master/courses/dl2/wgan.ipynb\n",
    "def train(n_iter, alternate=False, first=False):\n",
    "    gen_iters = 0\n",
    "    for epoch in trange(n_iter):\n",
    "        gan.train()\n",
    "        data_iter = iter(md.trn_dl)\n",
    "        i, n = 0, len(md.trn_dl)\n",
    "        \n",
    "        def train_G():\n",
    "            ''' Train generator '''\n",
    "            nonlocal gen_iters\n",
    "            \n",
    "            fastai.core.set_trainable(gan.D, False)\n",
    "            fastai.core.set_trainable(gan.G, True)\n",
    "\n",
    "            gan.G.zero_grad()\n",
    "\n",
    "            #print(i, n)\n",
    "            loss_g = - torch.log(gan()).mean()\n",
    "            loss_g.backward()\n",
    "            opt_g.step()\n",
    "            gen_iters += 1\n",
    "            \n",
    "            return loss_g\n",
    "\n",
    "        def train_D():\n",
    "            ''' Train discriminator '''\n",
    "            nonlocal i\n",
    "            \n",
    "            fastai.core.set_trainable(gan.D, True)\n",
    "            fastai.core.set_trainable(gan.G, False)\n",
    "            d_iters = 100 if (first and (gen_iters < 25) or (gen_iters % 500 == 0)) else 3\n",
    "            j = 0\n",
    "\n",
    "            while (j < d_iters) and (i < n):\n",
    "                j += 1; i += 1\n",
    "                batch = next(data_iter)\n",
    "                #print(j, i, batch[0].size(), batch[1].size())\n",
    "                emb_real = batch[0].to(DEVICE)\n",
    "                v = gan(emb_real).to(DEVICE)\n",
    "\n",
    "                gan.D.zero_grad()\n",
    "\n",
    "                #loss_d = - (batch[1] * torch.log(v) + (1.0-batch[1]) * torch.log(1.0 - v)).mean()\n",
    "                loss_d = - (batch[1].to(DEVICE) * torch.log(v) + (1.0-batch[1].to(DEVICE)) * torch.log(1.0 - v)).mean()\n",
    "                loss_d.backward()\n",
    "                opt_d.step()\n",
    "                \n",
    "                pbar.update()\n",
    "                \n",
    "            return loss_d\n",
    "        \n",
    "        with tqdm(total=n) as pbar:\n",
    "            while i < n:\n",
    "                if alternate:\n",
    "                    # train discriminator\n",
    "                    loss_d = train_D()\n",
    "                    # then train generator a little bit\n",
    "                    loss_g = train_G()\n",
    "                else:\n",
    "                    # train generator only\n",
    "                    i += 1\n",
    "                    loss_g = train_G()\n",
    "                    pbar.update()\n",
    "        \n",
    "        if alternate:\n",
    "            print(f'Loss_D {to_np(loss_d)}; Loss_G {to_np(loss_g)}; ')\n",
    "        else:\n",
    "            print(f'Loss_G {to_np(loss_g)}; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_generate(gan, n_epoch, genres=fixed_genres, alternate=False, n_sample=10):\n",
    "    train(n_epoch, alternate)\n",
    "    res = []\n",
    "    for i in range(n_sample):\n",
    "        res.append(generate(genres, gan))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 66.12it/s]\n",
      "Loss_D 0.019143156707286835; Loss_G 2.1032607555389404; \n",
      "100%|| 1/1 [00:00<00:00,  9.26it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['UNK is exactly what youd away hit the UNK in in UNK and today adding some dance music or enjoy',\n",
       " 'we on the interview with UNK back is a a speaks to me with the rest of the works',\n",
       " 'there feels have a stand on over UNK UNK and UNK by the example of its free stuff',\n",
       " 'french trio artist has a look at the UNK and the club in which features some social club than club body',\n",
       " 'to be taken from the brooklyn based trio artist returns with the type of and weve taken from the record season',\n",
       " 'i grew up the UNK UNK UNK UNK is so with the UNK and rather is just one below below',\n",
       " 'have pick up the UNK UNK UNK UNK UNK end of the UNK definitely check it below',\n",
       " 'artist has just dropped a brand new all over the UNK UNK UNK is back in the artwork for some ethereal atmosphere',\n",
       " 'aside from artist remix thats right look at the part of UNK UNK i have this fall tour dates below',\n",
       " 'new song that came out in the UNK and image remix of artist seems to send you into a familiar with each something']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res[0] for res in train_and_generate(gan, 1, alternate=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 40.44it/s]\n",
      "Loss_G 1.5611191987991333; \n",
      "100%|| 1/1 [00:00<00:00,  5.72it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['of the track to be taken from their debut ep whether this is a courtesy of last tune',\n",
       " 'UNK has teamed up with a cover of which better known as the she returns as an major label',\n",
       " 'i were quite familiar with the clean ep which is one of the image via dirty baby',\n",
       " 'UNK has teamed up up some of their debut UNK which is a song produced a ninja tune',\n",
       " 'the track is produced songstress artist and her vocals is described as much as its early work',\n",
       " 'the fact that i have teamed up with some of the ep which are described as a preview of their previous work',\n",
       " 'i was inspired by the sort of sultry vocals with the image courtesy of his previous age',\n",
       " 'of the the thing in UNK and some of the song to released earlier this year with his sophomore minutes',\n",
       " 'we are a bit of a track that theyre back in ep and the UNK is different of the player world',\n",
       " 'have released a kind of artist is the better known as artist returns with an excellent cover']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res[0] for res in train_and_generate(gan, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 53.88it/s]\n",
      "Loss_G 1.203406572341919; \n",
      "100%|| 1/1 [00:00<00:00,  7.60it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the that we fell in combined in the kind of vocals and described as a collaboration of human baby',\n",
       " 'some of their forthcoming debut ep which is combined with in and and the final track to keep your mid baby',\n",
       " 'an new remix of artist have hints at the UNK and with a UNK UNK is taken from his no ride',\n",
       " 'taken from the likes of UNK we have teamed along with the new night and just continues to keep your wild ride',\n",
       " 'in the part of new track called UNK which is the sort of below along as an press release',\n",
       " 'with some of the of artists such as they are built of a track and thats taken from his last career',\n",
       " 'the UNK UNK have teamed up with some of the UNK and in addition to this UNK ahead of their last career',\n",
       " 'when she combined with a debut and and in which is one of the super catchy from its final atmosphere',\n",
       " 'is the kind of UNK and has impressed returns with the part of a song weve holding on last career',\n",
       " 'the UNK is sort of sitting in and and the final happens to be taken from his major career']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res[0] for res in train_and_generate(gan, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 69.02it/s]\n",
      "Loss_G 0.9655078649520874; \n",
      "100%|| 1/1 [00:00<00:00,  9.62it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the canadian UNK UNK has teamed up with some of the UNK and as an impact on these ahead',\n",
       " 'this remix of who has grew up on the UNK and i to enjoy the track they enjoy',\n",
       " 'the fan of that is back of UNK and the UNK track to keep your eye on they ride',\n",
       " 'in my UNK has teamed up with some of the british from artist and its a kind of live ahead',\n",
       " 'UNK seems to step away on the millions of others and this is a moving hailing for ten ride',\n",
       " 'artist returns is the sort and kick back in UNK which features the sultry video for its ethereal ride',\n",
       " 'one of the UNK UNK UNK and is back at the pace which makes it all over this go ahead',\n",
       " 'this UNK a sound that is look at the game and killing it its an holding on previous ride',\n",
       " 'with their new ep which is just as some of the track and its an eye on wild enjoy',\n",
       " 'UNK UNK UNK UNK addition over the vocals and there is a bit departure of its final atmosphere']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res[0] for res in train_and_generate(gan, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as ganG_state.pt\n"
     ]
    }
   ],
   "source": [
    "def save():\n",
    "    save_state_filename = 'ganG_state.pt'\n",
    "    torch.save(gan.G.state_dict(), save_state_filename)\n",
    "    print('Saved as %s' % (save_state_filename))\n",
    "    \n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_generate(vae, condition, n_latent, ganG, max_sample=False, trunc_sample=True):\n",
    "    with torch.no_grad():\n",
    "        ganG.eval()\n",
    "        z = torch.randn(1, n_latent).to(DEVICE)\n",
    "        decode_embed = to_embed(z, condition).to(DEVICE)\n",
    "        z_prime = ganG(decode_embed)\n",
    "\n",
    "        generated = vae.decoder.generate_with_embed(z_prime, 50, temp, DEVICE, max_sample=max_sample, trunc_sample=trunc_sample)\n",
    "        generated_str = model.float_word_tensor_to_string(output_side, generated)\n",
    "\n",
    "        EOS_str = f' {output_side.index_to_word(torch.LongTensor([EOS_token]))} '\n",
    "\n",
    "        if generated_str.endswith(EOS_str):\n",
    "            generated_str = generated_str[:-5]\n",
    "\n",
    "        # flip it back\n",
    "        return generated_str[::-1], z, z_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a form of UNK and UNK is no wonder throughout a track just interested welcome addition to their music career',\n",
       " tensor([[ 0.3426, -0.4434,  0.6537, -1.0167,  0.2749,  0.3956, -1.0323,\n",
       "           1.9262, -0.2886,  0.0875, -1.1526,  0.5510, -0.6338, -0.5600,\n",
       "          -1.1005, -0.0089, -0.0115,  0.3233, -1.4833, -0.4620,  0.2851,\n",
       "          -0.1228, -2.3361, -1.1106,  0.3530,  0.6186, -0.2250, -1.9346,\n",
       "           0.4420, -1.3540, -0.9850,  1.5157,  1.8120,  0.8877, -1.1195,\n",
       "           0.1646,  0.2872, -1.1713, -0.0716, -0.2902, -0.3561,  0.2219,\n",
       "           0.8907, -1.1704, -0.0048, -0.6173,  1.3518,  0.4813, -0.1438,\n",
       "          -1.4539,  0.2248, -0.2255,  1.8956,  0.4242,  0.5299, -0.6540,\n",
       "           0.7414, -0.7336, -0.3256,  1.0842,  0.2970, -1.3575, -1.0421,\n",
       "          -1.7367, -0.6553,  0.8228,  1.5621, -0.5879, -0.6136, -0.1467,\n",
       "          -0.0047,  1.4218,  1.6615,  0.0985, -0.0368, -0.6737,  0.0706,\n",
       "          -1.5198, -0.6521, -1.7595,  0.6300,  2.0502,  1.0153, -0.0525,\n",
       "           0.5323, -0.1132, -1.6511, -1.0310,  0.3586,  1.0938,  1.1957,\n",
       "          -0.3782, -0.0399,  0.4224, -0.3741,  0.7597, -1.6788, -0.3057,\n",
       "          -2.3552,  0.7812, -0.7370,  1.1828, -1.2971, -0.9342,  0.2744,\n",
       "           1.1596, -1.3242,  0.8534,  0.3953, -0.1695, -0.5580,  2.6126,\n",
       "          -1.2040,  1.5317,  1.2366,  0.6129,  0.1540,  0.7908, -0.5675,\n",
       "           0.9648, -0.1355, -1.2290,  0.1643, -0.7062, -0.3818,  1.3383,\n",
       "           2.5146, -1.3538]]),\n",
       " tensor([[-0.1997, -0.3933, -0.1366,  0.2896, -0.2694,  0.5554, -0.4291,\n",
       "           0.1784, -0.8145,  0.3567, -0.1028, -0.5704, -0.5558, -1.0269,\n",
       "          -0.7751, -0.6952, -0.3344, -0.5245,  0.2302, -0.3832,  0.7509,\n",
       "          -0.5724, -1.0788, -0.1120,  0.9704,  0.4537, -0.3294,  0.0231,\n",
       "           0.3627, -0.8513, -0.5363, -0.2863,  1.2027,  0.5495, -0.2897,\n",
       "          -0.1780,  0.6128, -0.1126,  0.0109, -0.0178, -0.6538, -0.4250,\n",
       "           0.4682, -0.7202,  0.5161, -0.5888,  0.1093,  0.6071, -0.1942,\n",
       "          -0.3713, -0.1983, -0.0872,  0.4530,  0.6409,  0.4646, -0.6379,\n",
       "          -0.2515, -0.7116, -0.4701,  0.8013,  1.0349,  0.0460, -0.0475,\n",
       "          -0.4473,  0.2891,  0.4093,  0.4159, -0.6048,  0.9763, -0.3218,\n",
       "          -0.0990,  0.2018,  1.2032, -0.3678, -0.3175, -0.0047, -0.2882,\n",
       "          -0.4269, -0.6650, -0.5275,  0.9563,  0.3910,  0.7186, -0.1128,\n",
       "          -0.5424,  0.4401,  0.0617,  0.0386,  0.1689,  0.4230,  1.2390,\n",
       "          -0.0521, -0.0004, -0.2969, -0.2484,  0.1569,  0.1672,  0.3974,\n",
       "           0.1078, -0.1779, -0.5191,  0.3782, -0.3793, -0.4822,  0.7008,\n",
       "           0.2482, -0.4773,  0.6696, -0.5353,  0.5425, -0.2317,  0.0329,\n",
       "          -0.1983,  0.5910,  0.2821, -0.0809,  0.8051, -0.0266,  0.1180,\n",
       "           0.4812, -0.0489, -0.5433,  0.1834, -0.0136, -0.5206, -0.5820,\n",
       "           1.2708, -0.7500, -0.3894, -0.0502,  0.4899,  0.7575,  0.1492,\n",
       "          -0.9052, -0.1464,  0.3496, -0.4562, -1.0029,  0.6765, -0.8423,\n",
       "           0.6625, -0.5008,  0.0227,  0.4649]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ganG = LCGAN_G(embed_size).to(DEVICE)\n",
    "ganG.load_state_dict(torch.load('ganG_state.pt'))\n",
    "\n",
    "gan_generate(vae, torch.FloatTensor(dataset.encode_genres(['hip hop','pop','pop rap','rap','trap music'])).to(DEVICE), n_latent, ganG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrain discriminator with new samples... `z_prime`s that the discriminator is still not rejecting strongly enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.ones(len(g2), dtype=int)\n",
    "gens_lose = list(set([i for b in banned for i in np.where([b in res[0].split() for res in g2])[0]]))\n",
    "#gens_keep = list(set(range(len(g2))) - gens_lose)\n",
    "labels[gens_lose] = -1\n",
    "zs_keep = np.array([res[2] for res in g2], dtype=object)\n",
    "\n",
    "labels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LatentDataset(zs_keep, labels.astype(float))\n",
    "dl = DataLoader(ds, batch_size)\n",
    "md = ModelData('.', dl, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:00<00:00, 284.11it/s]\n",
      "Loss_D 1.193210244178772; Loss_G 1.1896806955337524; \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['artist has teams up with a a one of the UNK and it would be taken from i don no feat',\n",
       " 'this with a remix of UNK and trying to take of the UNK theres no feat',\n",
       " 'the remix of UNK has been trying to take of the UNK theres no feat',\n",
       " 'UNK is a listen to the UNK and coming side of what to look at feat',\n",
       " 'a new track called UNK and serves as one of the trying would be ready for at times',\n",
       " 'artist has up with a UNK and one of the track will be inspired as no feat',\n",
       " 'artist is back in the likes of UNK and serves as a no times',\n",
       " 'artist who reminds back to a more stuck on the UNK and theres many feat',\n",
       " 'he continues to serves as one of the week what i look no feat',\n",
       " 'with a layers of UNK and and UNK of the perfect trying i look at feat']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[res[0] for res in train_and_generate(gan, 1, alternate=True)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
